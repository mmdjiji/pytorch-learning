{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "PyTorch 实践\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using { device } device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# 构建自己的数据集\n",
    "class MnistDataset(Dataset):\n",
    "  def __init__(self, transform=None, path=None):\n",
    "    self.path = path\n",
    "    self.data = os.listdir(self.path)\n",
    "    self.transform = transform\n",
    "    self.len = len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    image_index = self.data[index]\n",
    "    img_path = os.path.join(self.path, image_index)\n",
    "    img = Image.open(img_path)\n",
    "    if self.transform:\n",
    "      img = self.transform(img)\n",
    "\n",
    "    label = int(image_index[-5])\n",
    "    # label = self.oneHot(label)\n",
    "    return img, label\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.len\n",
    "\n",
    "  # 将标签转为onehot编码\n",
    "  def oneHot(self, label):\n",
    "    tem = np.zeros(10)\n",
    "    tem[label] = 1\n",
    "    return torch.from_numpy(tem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# 定义网络\n",
    "class NeuralNetwork(nn.Module):\n",
    "  # def __init__(self):\n",
    "  #   super(NeuralNetwork, self).__init__()\n",
    "  #   self.flatten = nn.Flatten()\n",
    "  #   self.linear_relu_stack = nn.Sequential(\n",
    "  #     nn.Linear(in_features=28 * 28, out_features=512),\n",
    "  #     nn.ReLU(),\n",
    "  #     nn.Linear(in_features=512, out_features=512),\n",
    "  #     nn.ReLU(),\n",
    "  #     nn.Linear(in_features=512, out_features=10),\n",
    "  #   )\n",
    "  # def forward(self, x):\n",
    "  #   x = self.flatten(x)\n",
    "  #   logits = self.linear_relu_stack(x)\n",
    "  #   return logits\n",
    "\n",
    "  def __init__(self):\n",
    "    super(NeuralNetwork, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "    self.conv2_drop = nn.Dropout2d()\n",
    "    self.fc1 = nn.Linear(320, 50)\n",
    "    self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "    x = x.view(-1, 320)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.dropout(x, training=self.training)\n",
    "    x = self.fc2(x)\n",
    "    return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化模型参数\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "  size = len(dataloader.dataset)\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "    # 前向传播，计算预测值\n",
    "    pred = model(X)\n",
    "    # 计算损失\n",
    "    loss = loss_fn(pred, y)\n",
    "    # 反向传播，优化参数\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      loss, current = loss.item(), batch * len(X)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# 测试模型性能\n",
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "      # 前向传播，计算预测值\n",
    "      pred = model(X)\n",
    "      \n",
    "      # 计算损失\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "      # 计算准确率\n",
    "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "  test_loss /= num_batches\n",
    "  correct /= size\n",
    "  print(f\"Test: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiji/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.297560  [    0/60000]\n",
      "loss: 2.301487  [ 6400/60000]\n",
      "loss: 2.307737  [12800/60000]\n",
      "loss: 2.301557  [19200/60000]\n",
      "loss: 2.319473  [25600/60000]\n",
      "loss: 2.308531  [32000/60000]\n",
      "loss: 2.291257  [38400/60000]\n",
      "loss: 2.296378  [44800/60000]\n",
      "loss: 2.292051  [51200/60000]\n",
      "loss: 2.306219  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 12.0%, Avg loss: 2.296868 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.291897  [    0/60000]\n",
      "loss: 2.305747  [ 6400/60000]\n",
      "loss: 2.299816  [12800/60000]\n",
      "loss: 2.290524  [19200/60000]\n",
      "loss: 2.300469  [25600/60000]\n",
      "loss: 2.297340  [32000/60000]\n",
      "loss: 2.308899  [38400/60000]\n",
      "loss: 2.284355  [44800/60000]\n",
      "loss: 2.280794  [51200/60000]\n",
      "loss: 2.303311  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 14.6%, Avg loss: 2.287034 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.283633  [    0/60000]\n",
      "loss: 2.268251  [ 6400/60000]\n",
      "loss: 2.292315  [12800/60000]\n",
      "loss: 2.268906  [19200/60000]\n",
      "loss: 2.288314  [25600/60000]\n",
      "loss: 2.303162  [32000/60000]\n",
      "loss: 2.279492  [38400/60000]\n",
      "loss: 2.255013  [44800/60000]\n",
      "loss: 2.274033  [51200/60000]\n",
      "loss: 2.270479  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 16.4%, Avg loss: 2.272295 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.269140  [    0/60000]\n",
      "loss: 2.288998  [ 6400/60000]\n",
      "loss: 2.269589  [12800/60000]\n",
      "loss: 2.253817  [19200/60000]\n",
      "loss: 2.250832  [25600/60000]\n",
      "loss: 2.240482  [32000/60000]\n",
      "loss: 2.236519  [38400/60000]\n",
      "loss: 2.248220  [44800/60000]\n",
      "loss: 2.260531  [51200/60000]\n",
      "loss: 2.241153  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 18.5%, Avg loss: 2.236117 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.213945  [    0/60000]\n",
      "loss: 2.228632  [ 6400/60000]\n",
      "loss: 2.224497  [12800/60000]\n",
      "loss: 2.218624  [19200/60000]\n",
      "loss: 2.187624  [25600/60000]\n",
      "loss: 2.199863  [32000/60000]\n",
      "loss: 2.178286  [38400/60000]\n",
      "loss: 2.235423  [44800/60000]\n",
      "loss: 2.186485  [51200/60000]\n",
      "loss: 2.154787  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 25.4%, Avg loss: 2.143706 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.118187  [    0/60000]\n",
      "loss: 2.201222  [ 6400/60000]\n",
      "loss: 2.108903  [12800/60000]\n",
      "loss: 2.056077  [19200/60000]\n",
      "loss: 2.034467  [25600/60000]\n",
      "loss: 2.067208  [32000/60000]\n",
      "loss: 1.985400  [38400/60000]\n",
      "loss: 2.026709  [44800/60000]\n",
      "loss: 1.978311  [51200/60000]\n",
      "loss: 1.909771  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 31.4%, Avg loss: 1.929878 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.006870  [    0/60000]\n",
      "loss: 1.941455  [ 6400/60000]\n",
      "loss: 1.813224  [12800/60000]\n",
      "loss: 1.875610  [19200/60000]\n",
      "loss: 1.889923  [25600/60000]\n",
      "loss: 1.787774  [32000/60000]\n",
      "loss: 1.739074  [38400/60000]\n",
      "loss: 1.644117  [44800/60000]\n",
      "loss: 1.614978  [51200/60000]\n",
      "loss: 1.705952  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 36.9%, Avg loss: 1.666441 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.667278  [    0/60000]\n",
      "loss: 1.727701  [ 6400/60000]\n",
      "loss: 1.639600  [12800/60000]\n",
      "loss: 1.560993  [19200/60000]\n",
      "loss: 1.636475  [25600/60000]\n",
      "loss: 1.478743  [32000/60000]\n",
      "loss: 1.452650  [38400/60000]\n",
      "loss: 1.534890  [44800/60000]\n",
      "loss: 1.653199  [51200/60000]\n",
      "loss: 1.629034  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 43.4%, Avg loss: 1.470433 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.488934  [    0/60000]\n",
      "loss: 1.417668  [ 6400/60000]\n",
      "loss: 1.441068  [12800/60000]\n",
      "loss: 1.385018  [19200/60000]\n",
      "loss: 1.447002  [25600/60000]\n",
      "loss: 1.212989  [32000/60000]\n",
      "loss: 1.348150  [38400/60000]\n",
      "loss: 1.402247  [44800/60000]\n",
      "loss: 1.455891  [51200/60000]\n",
      "loss: 1.260799  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 48.5%, Avg loss: 1.338951 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.245152  [    0/60000]\n",
      "loss: 1.233569  [ 6400/60000]\n",
      "loss: 1.172126  [12800/60000]\n",
      "loss: 1.316716  [19200/60000]\n",
      "loss: 1.347971  [25600/60000]\n",
      "loss: 1.207623  [32000/60000]\n",
      "loss: 1.264025  [38400/60000]\n",
      "loss: 1.319689  [44800/60000]\n",
      "loss: 1.124240  [51200/60000]\n",
      "loss: 1.187822  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 52.0%, Avg loss: 1.255181 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.401478  [    0/60000]\n",
      "loss: 1.307785  [ 6400/60000]\n",
      "loss: 1.241590  [12800/60000]\n",
      "loss: 1.296381  [19200/60000]\n",
      "loss: 1.077773  [25600/60000]\n",
      "loss: 1.101931  [32000/60000]\n",
      "loss: 1.189397  [38400/60000]\n",
      "loss: 1.196919  [44800/60000]\n",
      "loss: 1.222000  [51200/60000]\n",
      "loss: 1.198466  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 54.7%, Avg loss: 1.189695 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.201229  [    0/60000]\n",
      "loss: 1.234716  [ 6400/60000]\n",
      "loss: 1.263004  [12800/60000]\n",
      "loss: 1.013810  [19200/60000]\n",
      "loss: 1.160106  [25600/60000]\n",
      "loss: 1.070325  [32000/60000]\n",
      "loss: 1.315727  [38400/60000]\n",
      "loss: 0.970340  [44800/60000]\n",
      "loss: 1.127987  [51200/60000]\n",
      "loss: 1.098701  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 57.8%, Avg loss: 1.124899 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.029683  [    0/60000]\n",
      "loss: 1.226798  [ 6400/60000]\n",
      "loss: 1.113429  [12800/60000]\n",
      "loss: 1.114764  [19200/60000]\n",
      "loss: 1.181064  [25600/60000]\n",
      "loss: 1.171647  [32000/60000]\n",
      "loss: 1.146292  [38400/60000]\n",
      "loss: 1.121845  [44800/60000]\n",
      "loss: 1.074849  [51200/60000]\n",
      "loss: 1.303634  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 59.1%, Avg loss: 1.093727 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 1.182286  [    0/60000]\n",
      "loss: 1.020613  [ 6400/60000]\n",
      "loss: 1.046603  [12800/60000]\n",
      "loss: 1.038924  [19200/60000]\n",
      "loss: 0.979865  [25600/60000]\n",
      "loss: 1.027406  [32000/60000]\n",
      "loss: 1.136024  [38400/60000]\n",
      "loss: 0.918380  [44800/60000]\n",
      "loss: 1.057462  [51200/60000]\n",
      "loss: 0.960808  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 60.2%, Avg loss: 1.066234 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.016585  [    0/60000]\n",
      "loss: 1.025829  [ 6400/60000]\n",
      "loss: 1.059161  [12800/60000]\n",
      "loss: 1.096182  [19200/60000]\n",
      "loss: 0.873059  [25600/60000]\n",
      "loss: 1.181438  [32000/60000]\n",
      "loss: 0.904582  [38400/60000]\n",
      "loss: 1.016790  [44800/60000]\n",
      "loss: 0.961437  [51200/60000]\n",
      "loss: 0.888448  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 61.3%, Avg loss: 1.041124 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.079430  [    0/60000]\n",
      "loss: 0.998069  [ 6400/60000]\n",
      "loss: 0.804220  [12800/60000]\n",
      "loss: 0.868575  [19200/60000]\n",
      "loss: 0.801551  [25600/60000]\n",
      "loss: 0.961224  [32000/60000]\n",
      "loss: 0.913847  [38400/60000]\n",
      "loss: 0.964220  [44800/60000]\n",
      "loss: 0.944366  [51200/60000]\n",
      "loss: 0.873819  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 62.6%, Avg loss: 1.021904 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.982026  [    0/60000]\n",
      "loss: 0.789918  [ 6400/60000]\n",
      "loss: 0.841475  [12800/60000]\n",
      "loss: 0.873565  [19200/60000]\n",
      "loss: 1.035563  [25600/60000]\n",
      "loss: 0.953151  [32000/60000]\n",
      "loss: 0.738946  [38400/60000]\n",
      "loss: 1.100679  [44800/60000]\n",
      "loss: 0.980819  [51200/60000]\n",
      "loss: 1.026116  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 63.1%, Avg loss: 0.993731 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.982404  [    0/60000]\n",
      "loss: 0.994290  [ 6400/60000]\n",
      "loss: 1.042132  [12800/60000]\n",
      "loss: 0.756713  [19200/60000]\n",
      "loss: 0.909280  [25600/60000]\n",
      "loss: 0.977959  [32000/60000]\n",
      "loss: 0.792122  [38400/60000]\n",
      "loss: 0.983559  [44800/60000]\n",
      "loss: 0.851126  [51200/60000]\n",
      "loss: 1.059830  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 63.9%, Avg loss: 0.977044 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.026505  [    0/60000]\n",
      "loss: 0.831711  [ 6400/60000]\n",
      "loss: 0.991423  [12800/60000]\n",
      "loss: 1.073277  [19200/60000]\n",
      "loss: 0.868326  [25600/60000]\n",
      "loss: 0.930772  [32000/60000]\n",
      "loss: 0.965013  [38400/60000]\n",
      "loss: 0.890170  [44800/60000]\n",
      "loss: 0.762289  [51200/60000]\n",
      "loss: 0.822786  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 64.4%, Avg loss: 0.972004 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.844205  [    0/60000]\n",
      "loss: 0.708459  [ 6400/60000]\n",
      "loss: 0.968169  [12800/60000]\n",
      "loss: 1.213675  [19200/60000]\n",
      "loss: 0.889109  [25600/60000]\n",
      "loss: 0.933483  [32000/60000]\n",
      "loss: 0.926900  [38400/60000]\n",
      "loss: 1.026772  [44800/60000]\n",
      "loss: 0.845109  [51200/60000]\n",
      "loss: 1.048036  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 64.8%, Avg loss: 0.952325 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.789120  [    0/60000]\n",
      "loss: 1.027675  [ 6400/60000]\n",
      "loss: 0.736719  [12800/60000]\n",
      "loss: 1.297206  [19200/60000]\n",
      "loss: 0.918375  [25600/60000]\n",
      "loss: 0.768132  [32000/60000]\n",
      "loss: 1.002601  [38400/60000]\n",
      "loss: 0.828507  [44800/60000]\n",
      "loss: 1.070912  [51200/60000]\n",
      "loss: 1.323433  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 65.6%, Avg loss: 0.938260 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.922192  [    0/60000]\n",
      "loss: 0.976004  [ 6400/60000]\n",
      "loss: 0.864585  [12800/60000]\n",
      "loss: 1.094485  [19200/60000]\n",
      "loss: 0.991763  [25600/60000]\n",
      "loss: 0.818746  [32000/60000]\n",
      "loss: 1.077942  [38400/60000]\n",
      "loss: 0.757967  [44800/60000]\n",
      "loss: 1.037694  [51200/60000]\n",
      "loss: 1.086248  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 66.2%, Avg loss: 0.928612 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.861507  [    0/60000]\n",
      "loss: 1.191224  [ 6400/60000]\n",
      "loss: 0.922613  [12800/60000]\n",
      "loss: 1.100310  [19200/60000]\n",
      "loss: 0.859791  [25600/60000]\n",
      "loss: 0.937472  [32000/60000]\n",
      "loss: 0.711483  [38400/60000]\n",
      "loss: 0.861968  [44800/60000]\n",
      "loss: 0.977782  [51200/60000]\n",
      "loss: 1.019930  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 66.2%, Avg loss: 0.924501 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.907140  [    0/60000]\n",
      "loss: 0.897745  [ 6400/60000]\n",
      "loss: 0.974523  [12800/60000]\n",
      "loss: 0.912743  [19200/60000]\n",
      "loss: 0.850688  [25600/60000]\n",
      "loss: 0.804906  [32000/60000]\n",
      "loss: 0.903680  [38400/60000]\n",
      "loss: 0.873782  [44800/60000]\n",
      "loss: 0.653956  [51200/60000]\n",
      "loss: 0.898894  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 65.6%, Avg loss: 0.913099 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.798614  [    0/60000]\n",
      "loss: 1.145831  [ 6400/60000]\n",
      "loss: 0.847966  [12800/60000]\n",
      "loss: 0.838122  [19200/60000]\n",
      "loss: 0.943784  [25600/60000]\n",
      "loss: 0.953649  [32000/60000]\n",
      "loss: 0.919873  [38400/60000]\n",
      "loss: 0.767958  [44800/60000]\n",
      "loss: 0.755503  [51200/60000]\n",
      "loss: 1.158397  [57600/60000]\n",
      "Test: \n",
      " Accuracy: 66.5%, Avg loss: 0.907010 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.778709  [    0/60000]\n",
      "loss: 0.976688  [ 6400/60000]\n",
      "loss: 0.838272  [12800/60000]\n",
      "loss: 1.147072  [19200/60000]\n",
      "loss: 0.863838  [25600/60000]\n",
      "loss: 0.819685  [32000/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b73637506177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t + 1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d67a7719cdbd>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# 前向传播，计算预测值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 计算损失\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-07c41963b943>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  print(f\"Using {device} device\")\n",
    "  # 定义模型\n",
    "  model = NeuralNetwork().to(device)\n",
    "  # 设置超参数\n",
    "  learning_rate = 1e-3\n",
    "  batch_size = 64\n",
    "  epochs = 50\n",
    "\n",
    "  train_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()  # 对样本数据进行处理，转换为张量数据\n",
    "  )\n",
    "  # 测试数据集\n",
    "  test_dataset = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()  # 对样本数据进行处理，转换为张量数据\n",
    "  )\n",
    "\n",
    "  train_data = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    # 设置批量大小\n",
    "    batch_size=64,\n",
    "    # 打乱样本的顺序\n",
    "    shuffle=True)\n",
    "  # 测试数据加载器\n",
    "  test_data = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True)\n",
    "\n",
    "  # 加载数据集\n",
    "  # train_dataset = MnistDataset(transform=ToTensor(), path='./mnist/train')\n",
    "  # train_data = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  # test_dataset = MnistDataset(transform=ToTensor(), path='./mnist/test')\n",
    "  # test_data = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  # 定义损失函数和优化器\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.SGD(params=model.parameters(), lr=learning_rate)\n",
    "  # 训练模型\n",
    "  for t in range(epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    train_loop(train_data, model, loss_fn, optimizer, device)\n",
    "    test_loop(test_data, model, loss_fn, device)\n",
    "  print(\"Done!\")\n",
    "  # 保存模型\n",
    "  torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa626c12245a67512840b3cbaf0cae59291798977c30804492c558ce3d5f8947"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
