{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTorch \u00b6 An open source machine learning framework that accelerates the path from research prototyping to production deployment. \u7531 Facebook \u4eba\u5de5\u667a\u80fd\u7814\u7a76\u5c0f\u7ec4\u5f00\u53d1\u7684\u4e00\u79cd\u57fa\u4e8e Lua \u7f16\u5199\u7684 Torch \u5e93\u7684 Python \u5b9e\u73b0\u7684\u6df1\u5ea6\u5b66\u4e60\u5e93\u3002 \u5b98\u7f51 | GitHub \u5b89\u88c5 \u00b6 https://pytorch.org/get-started/locally/ \u4ee3\u7801 \u00b6 https://github.com/mmdjiji/pytorch-learning/tree/main/codes","title":"\u7b80\u4ecb"},{"location":"#pytorch","text":"An open source machine learning framework that accelerates the path from research prototyping to production deployment. \u7531 Facebook \u4eba\u5de5\u667a\u80fd\u7814\u7a76\u5c0f\u7ec4\u5f00\u53d1\u7684\u4e00\u79cd\u57fa\u4e8e Lua \u7f16\u5199\u7684 Torch \u5e93\u7684 Python \u5b9e\u73b0\u7684\u6df1\u5ea6\u5b66\u4e60\u5e93\u3002 \u5b98\u7f51 | GitHub","title":"PyTorch"},{"location":"#_1","text":"https://pytorch.org/get-started/locally/","title":"\u5b89\u88c5"},{"location":"#_2","text":"https://github.com/mmdjiji/pytorch-learning/tree/main/codes","title":"\u4ee3\u7801"},{"location":"basic/","text":"\u57fa\u7840 \u00b6 \u7ebf\u6027\u4ee3\u6570 \u00b6 \u5f20\u91cf\uff08Tensor\uff09 \u00b6 0\u7ef4\u5f20\u91cf\uff08\u6807\u91cf\uff09 1\u7ef4\u5f20\u91cf\uff08\u5411\u91cf\uff09 2\u7ef4\u5f20\u91cf\uff08\u77e9\u9635\uff09 3\u7ef4\u5f20\u91cf\uff08\u65f6\u95f4\u5e8f\u5217\uff09 4\u7ef4\u5f20\u91cf\uff08\u56fe\u50cf\uff09 5\u7ef4\u5f20\u91cf\uff08\u89c6\u9891\uff09 import torch import numpy as np data = [[ 1 , 2 ], [ 3 , 4 ]] x_data = torch . tensor ( data ) print ( f \"Tensor from Data: \\n { x_data } \\n \" ) np_array = np . array ( data ) x_np = torch . from_numpy ( np_array ) print ( f \"Tensor from Numpy: \\n { x_np } \\n \" ) # \u4fdd\u7559\u539f\u6709\u5f20\u91cf\u7684\u5f62\u72b6\u548c\u6570\u636e\u7c7b\u578b\uff0c\u586b\u51451 x_ones = torch . ones_like ( x_data ) print ( f \"Ones Tensor: \\n { x_ones } \\n \" ) # \u663e\u5f0f\u66f4\u6539\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b\uff0c\u968f\u673a\u586b\u5145 x_rand = torch . rand_like ( x_data , dtype = torch . float ) print ( f \"Random Tensor: \\n { x_rand } \\n \" ) # \u521b\u5efa2\u884c3\u5217\u7684\u5f20\u91cf shape = ( 2 , 3 ) rand_tensor = torch . rand ( shape ) ones_tensor = torch . ones ( shape ) zeros_tensor = torch . zeros ( shape ) print ( f \"Random Tensor: \\n { rand_tensor } \\n \" ) print ( f \"Ones Tensor: \\n { ones_tensor } \\n \" ) print ( f \"Zeros Tensor: \\n { zeros_tensor } \\n \" ) # \u5c06\u5f20\u91cf\u79fb\u52a8\u5230GPU\u4e0a if torch . cuda . is_available (): tensor = tensor . to ( \"cuda\" ) print ( 'success moving to gpu' ) else : print ( 'failed moving to gpu, use cpu only' ) tensor = torch . ones ( 4 , 4 ) print ( f \"First row: { tensor [ 0 ] } \" ) print ( f \"First column: { tensor [:, 0 ] } \" ) print ( f \"Last column: { tensor [ ... , - 1 ] } \" ) tensor [:, 1 ] = 0 print ( tensor ) # \u5728\u7b2c1\u4e2a\u7ef4\u5ea6\u62fc\u63a5\uff0c\u5373\u6c34\u5e73\u65b9\u5411 t1 = torch . cat ([ tensor , tensor , tensor ], dim = 1 ) print ( t1 ) # \u77e9\u9635\u76f8\u4e58\uff0cy1\u3001y2\u548cy3\u7684\u503c\u76f8\u540c y1 = tensor @ tensor . T y2 = tensor . matmul ( tensor . T ) y3 = torch . rand_like ( tensor ) torch . matmul ( tensor , tensor . T , out = y3 ) print ( y1 ) # \u77e9\u9635\u9010\u5143\u7d20\u76f8\u4e58\uff0cz1\u3001z2\u548cz3\u7684\u503c\u76f8\u540c z1 = tensor * tensor z2 = tensor . mul ( tensor ) z3 = torch . rand_like ( tensor ) torch . mul ( tensor , tensor , out = z3 ) print ( z1 ) # \u53ea\u6709\u4e00\u4e2a\u503c\u7684\u5f20\u91cf\uff0c\u53ef\u4ee5\u901a\u8fc7item\u5c5e\u6027\u8f6c\u6362\u4e3a\u6570\u503c agg = tensor . sum () agg_item = agg . item () print ( agg_item , type ( agg_item )) \u5728CPU\u4e0a\u7684\u5f20\u91cf\u548cNumPy\u6570\u7ec4\u5171\u4eab\u5b83\u4eec\u7684\u5185\u5b58\u4f4d\u7f6e\uff0c\u6539\u53d8\u4e00\u4e2a\u4f1a\u6539\u53d8\u53e6\u4e00\u4e2a\u3002 t = torch . ones ( 5 ) print ( f \"t: { t } \" ) n = t . numpy () print ( f \"n: { n } \" ) t . add_ ( 1 ) print ( f \"t: { t } \" ) print ( f \"n: { n } \" ) n = np . ones ( 5 ) print ( f \"n: { n } \" ) t = torch . from_numpy ( n ) print ( f \"t: { t } \" ) np . add ( n , 2 , out = n ) print ( f \"t: { t } \" ) print ( f \"n: { n } \" ) \u6570\u503c\u8ba1\u7b97 \u00b6 https://github.com/Theano/Theano https://github.com/aesara-devs/aesara \u6570\u636e\u96c6 \u00b6 \u52a0\u8f7d\u6570\u636e\u96c6 \u00b6 import torch from torch.utils.data import Dataset from torchvision import datasets from torchvision.transforms import ToTensor import matplotlib.pyplot as plt # \u8bad\u7ec3\u6570\u636e\u96c6 training_data = datasets . FashionMNIST ( root = \"data\" , # \u6570\u636e\u96c6\u4e0b\u8f7d\u8def\u5f84 train = True , # True\u4e3a\u8bad\u7ec3\u96c6\uff0cFalse\u4e3a\u6d4b\u8bd5\u96c6 download = True , # \u662f\u5426\u8981\u4e0b\u8f7d transform = ToTensor () # \u5bf9\u6837\u672c\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u8f6c\u6362\u4e3a\u5f20\u91cf\u6570\u636e ) # \u6d4b\u8bd5\u6570\u636e\u96c6 test_data = datasets . FashionMNIST ( root = \"data\" , train = False , download = True , transform = ToTensor () ) \u53ef\u89c6\u5316\uff1a # \u6807\u7b7e\u5b57\u5178\uff0c\u4e00\u4e2akey\u952e\u5bf9\u5e94\u4e00\u4e2alabel labels_map = { 0 : \"T-Shirt\" , 1 : \"Trouser\" , 2 : \"Pullover\" , 3 : \"Dress\" , 4 : \"Coat\" , 5 : \"Sandal\" , 6 : \"Shirt\" , 7 : \"Sneaker\" , 8 : \"Bag\" , 9 : \"Ankle Boot\" , } # \u8bbe\u7f6e\u753b\u5e03\u5927\u5c0f figure = plt . figure ( figsize = ( 8 , 8 )) cols , rows = 3 , 3 for i in range ( 1 , cols * rows + 1 ): # \u968f\u673a\u751f\u6210\u4e00\u4e2a\u7d22\u5f15 sample_idx = torch . randint ( len ( training_data ), size = ( 1 ,)) . item () # \u83b7\u53d6\u6837\u672c\u53ca\u5176\u5bf9\u5e94\u7684\u6807\u7b7e img , label = training_data [ sample_idx ] # \u6dfb\u52a0\u5b50\u56fe figure . add_subplot ( rows , cols , i ) # \u8bbe\u7f6e\u6807\u9898 plt . title ( labels_map [ label ]) # \u4e0d\u663e\u793a\u5750\u6807\u8f74 plt . axis ( \"off\" ) # \u663e\u793a\u7070\u5ea6\u56fe plt . imshow ( img . squeeze (), cmap = \"gray\" ) plt . show () \u81ea\u5b9a\u4e49\u6570\u636e\u96c6 \u00b6 \u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u9700\u8981\u7ee7\u627f Dataset \u7c7b\uff0c\u5e76\u5b9e\u73b0\u4e09\u4e2a\u51fd\u6570\uff1a __init__ \uff1a\u5b9e\u4f8b\u5316Dataset\u5bf9\u8c61\u65f6\u8fd0\u884c\uff0c\u5b8c\u6210\u521d\u59cb\u5316\u5de5\u4f5c\u3002 __len__ \uff1a\u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f\u3002 __getitem__ \uff1a\u6839\u636e\u7d22\u5f15\u8fd4\u56de\u4e00\u4e2a\u6837\u672c\uff08\u6570\u636e\u548c\u6807\u7b7e\uff09\u3002 import os import pandas as pd from torchvision.io import read_image class CustomImageDataset ( Dataset ): def __init__ ( self , annotations_file , img_dir , transform = None , target_transform = None ): # \u8bfb\u53d6\u6807\u7b7e\u6587\u4ef6 self . img_labels = pd . read_csv ( annotations_file ) # \u8bfb\u53d6\u56fe\u7247\u5b58\u50a8\u8def\u5f84 self . img_dir = img_dir # \u6570\u636e\u5904\u7406\u65b9\u6cd5 self . transform = transform # \u6807\u7b7e\u5904\u7406\u65b9\u6cd5 self . target_transform = target_transform def __len__ ( self ): return len ( self . img_labels ) def __getitem__ ( self , idx ): # \u5355\u5f20\u56fe\u7247\u8def\u5f84 img_path = os . path . join ( self . img_dir , self . img_labels . iloc [ idx , 0 ]) # \u8bfb\u53d6\u56fe\u7247 image = read_image ( img_path ) # \u83b7\u5f97\u5bf9\u5e94\u7684\u6807\u7b7e label = self . img_labels . iloc [ idx , 1 ] if self . transform : image = self . transform ( image ) if self . target_transform : label = self . target_transform ( label ) # \u8fd4\u56de\u4e00\u4e2a\u5143\u7ec4 return image , label \u6570\u636e\u52a0\u8f7d\u5668 \u00b6 torch.utils.data.DataLoader \u00b6 \u6839\u636e\u6570\u636e\u96c6\u751f\u6210\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684\u5bf9\u8c61\uff0c\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u3002 \u5e38\u7528\u53c2\u6570\uff1a dataset (Dataset) \uff1a\u5b9a\u4e49\u597d\u7684\u6570\u636e\u96c6\u3002 batch_size (int, optional)\uff1a\u6bcf\u6b21\u653e\u5165\u7f51\u7edc\u8bad\u7ec3\u7684\u6279\u6b21\u5927\u5c0f\uff0c\u9ed8\u8ba4\u4e3a1. shuffle (bool, optional) \uff1a\u662f\u5426\u6253\u4e71\u6570\u636e\u7684\u987a\u5e8f\uff0c\u9ed8\u8ba4\u4e3aFalse\u3002\u4e00\u822c\u8bad\u7ec3\u96c6\u8bbe\u7f6e\u4e3aTrue\uff0c\u6d4b\u8bd5\u96c6\u8bbe\u7f6e\u4e3aFalse\u3002 num_workers (int, optional) \uff1a\u7ebf\u7a0b\u6570\uff0c\u9ed8\u8ba4\u4e3a0\u3002\u5728Windows\u4e0b\u8bbe\u7f6e\u5927\u4e8e0\u7684\u6570\u53ef\u80fd\u4f1a\u62a5\u9519\u3002 drop_last (bool, optional) \uff1a\u662f\u5426\u4e22\u5f03\u6700\u540e\u4e00\u4e2a\u6279\u6b21\u7684\u6570\u636e\uff0c\u9ed8\u8ba4\u4e3aFalse\u3002 \u4e24\u4e2a\u5de5\u5177\u5305\uff0c\u53ef\u914d\u5408DataLoader\u4f7f\u7528\uff1a enumerate(iterable, start=0)\uff1a\u8f93\u5165\u662f\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684\u5bf9\u8c61\u548c\u4e0b\u6807\u7d22\u5f15\u5f00\u59cb\u503c\uff1b\u8fd4\u56de\u53ef\u8fed\u4ee3\u5bf9\u8c61\u7684\u4e0b\u6807\u7d22\u5f15\u548c\u6570\u636e\u672c\u8eab\u3002 tqdm(iterable)\uff1a\u8fdb\u5ea6\u6761\u53ef\u89c6\u5316\u5de5\u5177\u5305 from torch.utils.data import DataLoader data_loader = DataLoader ( dataset = MyDataset , batch_size = 16 , shuffle = True , num_workers = 0 , drop_last = False , ) \u52a0\u8f7d\u6570\u636e \u00b6 \u5728\u8bad\u7ec3\u6a21\u578b\u65f6\uff0c\u6211\u4eec\u901a\u5e38\u5e0c\u671b\u4ee5\u5c0f\u6279\u91cf\u7684\u5f62\u5f0f\u4f20\u9012\u6837\u672c\uff0c\u8fd9\u6837\u53ef\u4ee5\u51cf\u5c11\u6a21\u578b\u7684\u8fc7\u62df\u5408\u3002 from torch.utils.data import DataLoader train_dataloader = DataLoader ( dataset = training_data , # \u8bbe\u7f6e\u6279\u91cf\u5927\u5c0f batch_size = 64 , # \u6253\u4e71\u6837\u672c\u7684\u987a\u5e8f shuffle = True ) test_dataloader = DataLoader ( dataset = test_data , batch_size = 64 , shuffle = True ) \u904d\u5386 DataLoader \u00b6 \u5c06\u6570\u636e\u52a0\u8f7d\u5230DataLoader\u540e\uff0c\u6bcf\u6b21\u8fed\u4ee3\u4e00\u6279\u6837\u672c\u6570\u636e\u548c\u6807\u7b7e\uff08\u8fd9\u91cc\u6279\u91cf\u5927\u5c0f\u4e3a64\uff09\uff0c\u4e14\u6837\u672c\u987a\u5e8f\u662f\u88ab\u6253\u4e71\u7684\u3002 # \u5c55\u793a\u56fe\u7247\u548c\u6807\u7b7e train_features , train_labels = next ( iter ( train_dataloader )) # (B,N,H,W) print ( f \"Feature batch shape: { train_features . size () } \" ) print ( f \"Labels batch shape: { train_labels . size () } \" ) # \u83b7\u53d6\u7b2c\u4e00\u5f20\u56fe\u7247\uff0c\u53bb\u9664\u7b2c\u4e00\u4e2a\u6279\u91cf\u7ef4\u5ea6 img = train_features [ 0 ] . squeeze () label = train_labels [ 0 ] plt . imshow ( img , cmap = \"gray\" ) plt . show () print ( f \"Label: { label } \" )","title":"\u57fa\u7840"},{"location":"basic/#_1","text":"","title":"\u57fa\u7840"},{"location":"basic/#_2","text":"","title":"\u7ebf\u6027\u4ee3\u6570"},{"location":"basic/#tensor","text":"0\u7ef4\u5f20\u91cf\uff08\u6807\u91cf\uff09 1\u7ef4\u5f20\u91cf\uff08\u5411\u91cf\uff09 2\u7ef4\u5f20\u91cf\uff08\u77e9\u9635\uff09 3\u7ef4\u5f20\u91cf\uff08\u65f6\u95f4\u5e8f\u5217\uff09 4\u7ef4\u5f20\u91cf\uff08\u56fe\u50cf\uff09 5\u7ef4\u5f20\u91cf\uff08\u89c6\u9891\uff09 import torch import numpy as np data = [[ 1 , 2 ], [ 3 , 4 ]] x_data = torch . tensor ( data ) print ( f \"Tensor from Data: \\n { x_data } \\n \" ) np_array = np . array ( data ) x_np = torch . from_numpy ( np_array ) print ( f \"Tensor from Numpy: \\n { x_np } \\n \" ) # \u4fdd\u7559\u539f\u6709\u5f20\u91cf\u7684\u5f62\u72b6\u548c\u6570\u636e\u7c7b\u578b\uff0c\u586b\u51451 x_ones = torch . ones_like ( x_data ) print ( f \"Ones Tensor: \\n { x_ones } \\n \" ) # \u663e\u5f0f\u66f4\u6539\u5f20\u91cf\u7684\u6570\u636e\u7c7b\u578b\uff0c\u968f\u673a\u586b\u5145 x_rand = torch . rand_like ( x_data , dtype = torch . float ) print ( f \"Random Tensor: \\n { x_rand } \\n \" ) # \u521b\u5efa2\u884c3\u5217\u7684\u5f20\u91cf shape = ( 2 , 3 ) rand_tensor = torch . rand ( shape ) ones_tensor = torch . ones ( shape ) zeros_tensor = torch . zeros ( shape ) print ( f \"Random Tensor: \\n { rand_tensor } \\n \" ) print ( f \"Ones Tensor: \\n { ones_tensor } \\n \" ) print ( f \"Zeros Tensor: \\n { zeros_tensor } \\n \" ) # \u5c06\u5f20\u91cf\u79fb\u52a8\u5230GPU\u4e0a if torch . cuda . is_available (): tensor = tensor . to ( \"cuda\" ) print ( 'success moving to gpu' ) else : print ( 'failed moving to gpu, use cpu only' ) tensor = torch . ones ( 4 , 4 ) print ( f \"First row: { tensor [ 0 ] } \" ) print ( f \"First column: { tensor [:, 0 ] } \" ) print ( f \"Last column: { tensor [ ... , - 1 ] } \" ) tensor [:, 1 ] = 0 print ( tensor ) # \u5728\u7b2c1\u4e2a\u7ef4\u5ea6\u62fc\u63a5\uff0c\u5373\u6c34\u5e73\u65b9\u5411 t1 = torch . cat ([ tensor , tensor , tensor ], dim = 1 ) print ( t1 ) # \u77e9\u9635\u76f8\u4e58\uff0cy1\u3001y2\u548cy3\u7684\u503c\u76f8\u540c y1 = tensor @ tensor . T y2 = tensor . matmul ( tensor . T ) y3 = torch . rand_like ( tensor ) torch . matmul ( tensor , tensor . T , out = y3 ) print ( y1 ) # \u77e9\u9635\u9010\u5143\u7d20\u76f8\u4e58\uff0cz1\u3001z2\u548cz3\u7684\u503c\u76f8\u540c z1 = tensor * tensor z2 = tensor . mul ( tensor ) z3 = torch . rand_like ( tensor ) torch . mul ( tensor , tensor , out = z3 ) print ( z1 ) # \u53ea\u6709\u4e00\u4e2a\u503c\u7684\u5f20\u91cf\uff0c\u53ef\u4ee5\u901a\u8fc7item\u5c5e\u6027\u8f6c\u6362\u4e3a\u6570\u503c agg = tensor . sum () agg_item = agg . item () print ( agg_item , type ( agg_item )) \u5728CPU\u4e0a\u7684\u5f20\u91cf\u548cNumPy\u6570\u7ec4\u5171\u4eab\u5b83\u4eec\u7684\u5185\u5b58\u4f4d\u7f6e\uff0c\u6539\u53d8\u4e00\u4e2a\u4f1a\u6539\u53d8\u53e6\u4e00\u4e2a\u3002 t = torch . ones ( 5 ) print ( f \"t: { t } \" ) n = t . numpy () print ( f \"n: { n } \" ) t . add_ ( 1 ) print ( f \"t: { t } \" ) print ( f \"n: { n } \" ) n = np . ones ( 5 ) print ( f \"n: { n } \" ) t = torch . from_numpy ( n ) print ( f \"t: { t } \" ) np . add ( n , 2 , out = n ) print ( f \"t: { t } \" ) print ( f \"n: { n } \" )","title":"\u5f20\u91cf\uff08Tensor\uff09"},{"location":"basic/#_3","text":"https://github.com/Theano/Theano https://github.com/aesara-devs/aesara","title":"\u6570\u503c\u8ba1\u7b97"},{"location":"basic/#_4","text":"","title":"\u6570\u636e\u96c6"},{"location":"basic/#_5","text":"import torch from torch.utils.data import Dataset from torchvision import datasets from torchvision.transforms import ToTensor import matplotlib.pyplot as plt # \u8bad\u7ec3\u6570\u636e\u96c6 training_data = datasets . FashionMNIST ( root = \"data\" , # \u6570\u636e\u96c6\u4e0b\u8f7d\u8def\u5f84 train = True , # True\u4e3a\u8bad\u7ec3\u96c6\uff0cFalse\u4e3a\u6d4b\u8bd5\u96c6 download = True , # \u662f\u5426\u8981\u4e0b\u8f7d transform = ToTensor () # \u5bf9\u6837\u672c\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u8f6c\u6362\u4e3a\u5f20\u91cf\u6570\u636e ) # \u6d4b\u8bd5\u6570\u636e\u96c6 test_data = datasets . FashionMNIST ( root = \"data\" , train = False , download = True , transform = ToTensor () ) \u53ef\u89c6\u5316\uff1a # \u6807\u7b7e\u5b57\u5178\uff0c\u4e00\u4e2akey\u952e\u5bf9\u5e94\u4e00\u4e2alabel labels_map = { 0 : \"T-Shirt\" , 1 : \"Trouser\" , 2 : \"Pullover\" , 3 : \"Dress\" , 4 : \"Coat\" , 5 : \"Sandal\" , 6 : \"Shirt\" , 7 : \"Sneaker\" , 8 : \"Bag\" , 9 : \"Ankle Boot\" , } # \u8bbe\u7f6e\u753b\u5e03\u5927\u5c0f figure = plt . figure ( figsize = ( 8 , 8 )) cols , rows = 3 , 3 for i in range ( 1 , cols * rows + 1 ): # \u968f\u673a\u751f\u6210\u4e00\u4e2a\u7d22\u5f15 sample_idx = torch . randint ( len ( training_data ), size = ( 1 ,)) . item () # \u83b7\u53d6\u6837\u672c\u53ca\u5176\u5bf9\u5e94\u7684\u6807\u7b7e img , label = training_data [ sample_idx ] # \u6dfb\u52a0\u5b50\u56fe figure . add_subplot ( rows , cols , i ) # \u8bbe\u7f6e\u6807\u9898 plt . title ( labels_map [ label ]) # \u4e0d\u663e\u793a\u5750\u6807\u8f74 plt . axis ( \"off\" ) # \u663e\u793a\u7070\u5ea6\u56fe plt . imshow ( img . squeeze (), cmap = \"gray\" ) plt . show ()","title":"\u52a0\u8f7d\u6570\u636e\u96c6"},{"location":"basic/#_6","text":"\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u9700\u8981\u7ee7\u627f Dataset \u7c7b\uff0c\u5e76\u5b9e\u73b0\u4e09\u4e2a\u51fd\u6570\uff1a __init__ \uff1a\u5b9e\u4f8b\u5316Dataset\u5bf9\u8c61\u65f6\u8fd0\u884c\uff0c\u5b8c\u6210\u521d\u59cb\u5316\u5de5\u4f5c\u3002 __len__ \uff1a\u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f\u3002 __getitem__ \uff1a\u6839\u636e\u7d22\u5f15\u8fd4\u56de\u4e00\u4e2a\u6837\u672c\uff08\u6570\u636e\u548c\u6807\u7b7e\uff09\u3002 import os import pandas as pd from torchvision.io import read_image class CustomImageDataset ( Dataset ): def __init__ ( self , annotations_file , img_dir , transform = None , target_transform = None ): # \u8bfb\u53d6\u6807\u7b7e\u6587\u4ef6 self . img_labels = pd . read_csv ( annotations_file ) # \u8bfb\u53d6\u56fe\u7247\u5b58\u50a8\u8def\u5f84 self . img_dir = img_dir # \u6570\u636e\u5904\u7406\u65b9\u6cd5 self . transform = transform # \u6807\u7b7e\u5904\u7406\u65b9\u6cd5 self . target_transform = target_transform def __len__ ( self ): return len ( self . img_labels ) def __getitem__ ( self , idx ): # \u5355\u5f20\u56fe\u7247\u8def\u5f84 img_path = os . path . join ( self . img_dir , self . img_labels . iloc [ idx , 0 ]) # \u8bfb\u53d6\u56fe\u7247 image = read_image ( img_path ) # \u83b7\u5f97\u5bf9\u5e94\u7684\u6807\u7b7e label = self . img_labels . iloc [ idx , 1 ] if self . transform : image = self . transform ( image ) if self . target_transform : label = self . target_transform ( label ) # \u8fd4\u56de\u4e00\u4e2a\u5143\u7ec4 return image , label","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u96c6"},{"location":"basic/#_7","text":"","title":"\u6570\u636e\u52a0\u8f7d\u5668"},{"location":"basic/#torchutilsdatadataloader","text":"\u6839\u636e\u6570\u636e\u96c6\u751f\u6210\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684\u5bf9\u8c61\uff0c\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u3002 \u5e38\u7528\u53c2\u6570\uff1a dataset (Dataset) \uff1a\u5b9a\u4e49\u597d\u7684\u6570\u636e\u96c6\u3002 batch_size (int, optional)\uff1a\u6bcf\u6b21\u653e\u5165\u7f51\u7edc\u8bad\u7ec3\u7684\u6279\u6b21\u5927\u5c0f\uff0c\u9ed8\u8ba4\u4e3a1. shuffle (bool, optional) \uff1a\u662f\u5426\u6253\u4e71\u6570\u636e\u7684\u987a\u5e8f\uff0c\u9ed8\u8ba4\u4e3aFalse\u3002\u4e00\u822c\u8bad\u7ec3\u96c6\u8bbe\u7f6e\u4e3aTrue\uff0c\u6d4b\u8bd5\u96c6\u8bbe\u7f6e\u4e3aFalse\u3002 num_workers (int, optional) \uff1a\u7ebf\u7a0b\u6570\uff0c\u9ed8\u8ba4\u4e3a0\u3002\u5728Windows\u4e0b\u8bbe\u7f6e\u5927\u4e8e0\u7684\u6570\u53ef\u80fd\u4f1a\u62a5\u9519\u3002 drop_last (bool, optional) \uff1a\u662f\u5426\u4e22\u5f03\u6700\u540e\u4e00\u4e2a\u6279\u6b21\u7684\u6570\u636e\uff0c\u9ed8\u8ba4\u4e3aFalse\u3002 \u4e24\u4e2a\u5de5\u5177\u5305\uff0c\u53ef\u914d\u5408DataLoader\u4f7f\u7528\uff1a enumerate(iterable, start=0)\uff1a\u8f93\u5165\u662f\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684\u5bf9\u8c61\u548c\u4e0b\u6807\u7d22\u5f15\u5f00\u59cb\u503c\uff1b\u8fd4\u56de\u53ef\u8fed\u4ee3\u5bf9\u8c61\u7684\u4e0b\u6807\u7d22\u5f15\u548c\u6570\u636e\u672c\u8eab\u3002 tqdm(iterable)\uff1a\u8fdb\u5ea6\u6761\u53ef\u89c6\u5316\u5de5\u5177\u5305 from torch.utils.data import DataLoader data_loader = DataLoader ( dataset = MyDataset , batch_size = 16 , shuffle = True , num_workers = 0 , drop_last = False , )","title":"torch.utils.data.DataLoader"},{"location":"basic/#_8","text":"\u5728\u8bad\u7ec3\u6a21\u578b\u65f6\uff0c\u6211\u4eec\u901a\u5e38\u5e0c\u671b\u4ee5\u5c0f\u6279\u91cf\u7684\u5f62\u5f0f\u4f20\u9012\u6837\u672c\uff0c\u8fd9\u6837\u53ef\u4ee5\u51cf\u5c11\u6a21\u578b\u7684\u8fc7\u62df\u5408\u3002 from torch.utils.data import DataLoader train_dataloader = DataLoader ( dataset = training_data , # \u8bbe\u7f6e\u6279\u91cf\u5927\u5c0f batch_size = 64 , # \u6253\u4e71\u6837\u672c\u7684\u987a\u5e8f shuffle = True ) test_dataloader = DataLoader ( dataset = test_data , batch_size = 64 , shuffle = True )","title":"\u52a0\u8f7d\u6570\u636e"},{"location":"basic/#dataloader","text":"\u5c06\u6570\u636e\u52a0\u8f7d\u5230DataLoader\u540e\uff0c\u6bcf\u6b21\u8fed\u4ee3\u4e00\u6279\u6837\u672c\u6570\u636e\u548c\u6807\u7b7e\uff08\u8fd9\u91cc\u6279\u91cf\u5927\u5c0f\u4e3a64\uff09\uff0c\u4e14\u6837\u672c\u987a\u5e8f\u662f\u88ab\u6253\u4e71\u7684\u3002 # \u5c55\u793a\u56fe\u7247\u548c\u6807\u7b7e train_features , train_labels = next ( iter ( train_dataloader )) # (B,N,H,W) print ( f \"Feature batch shape: { train_features . size () } \" ) print ( f \"Labels batch shape: { train_labels . size () } \" ) # \u83b7\u53d6\u7b2c\u4e00\u5f20\u56fe\u7247\uff0c\u53bb\u9664\u7b2c\u4e00\u4e2a\u6279\u91cf\u7ef4\u5ea6 img = train_features [ 0 ] . squeeze () label = train_labels [ 0 ] plt . imshow ( img , cmap = \"gray\" ) plt . show () print ( f \"Label: { label } \" )","title":"\u904d\u5386 DataLoader"},{"location":"deep-learning/","text":"\u6df1\u5ea6\u5b66\u4e60 \u00b6 \u5386\u53f2 \u00b6 \u7ebf\u6027\u6a21\u578b\uff0820c50s\uff09 \u00b6 \u7b2c\u4e00\u6b21\u795e\u7ecf\u7f51\u7edc\u7814\u7a76\u6d6a\u6f6e\u79f0\u4e3a\u63a7\u5236\u8bba\u3002 \\[ f(x,w) = x_1w_1 + ... + x_nw_n \\] \\(n\\) \u7ec4\u8f93\u5165 \\(x_1, ..., x_n\\) \uff0c\u8f93\u51fa \\(y\\) \uff0c\u6743\u91cd \\(w_1, ..., w_n\\) \uff0c\u901a\u8fc7\u68c0\u9a8c\u51fd\u6570 \\(f(x,w)\\) \u7684\u6b63\u8d1f\u6765\u8bc6\u522b\u4e24\u79cd\u4e0d\u540c\u7c7b\u522b\u7684\u8f93\u5165\uff0c\u8fd9\u4e9b\u6743\u91cd\u7531\u64cd\u4f5c\u4eba\u5458\u8bbe\u5b9a\u3002 \u6700\u8457\u540d\u7684\u5c40\u9650\u6027\u662f\u65e0\u6cd5\u5b66\u4e60\u5f02\u6216\u3002 \u8054\u7ed3\u4e3b\u4e49\uff0820c80s\uff09 \u00b6 \u5206\u5e03\u5f0f\u8868\u793a\uff0c\u7cfb\u7edf\u4e2d\u7684\u6bcf\u4e00\u4e2a\u8f93\u5165\u90fd\u5e94\u8be5\u7531\u591a\u4e2a\u7279\u5f81\u8868\u793a\u3002 \u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\uff0c\u4ee5\u8fdb\u884c\u8bad\u7ec3\u3002 \u957f\u77ed\u671f\u8bb0\u5fc6\uff0820c90s\uff09 \u00b6 LSTM\uff0c\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u5efa\u6a21\u7684\u6570\u5b66\u95ee\u9898\u3002 \u8d2a\u5a6a\u9010\u5c42\u9884\u8bad\u7ec3\uff082006\uff09 \u00b6 Hinton et al., 2006a","title":"\u6df1\u5ea6\u5b66\u4e60"},{"location":"deep-learning/#_1","text":"","title":"\u6df1\u5ea6\u5b66\u4e60"},{"location":"deep-learning/#_2","text":"","title":"\u5386\u53f2"},{"location":"deep-learning/#20c50s","text":"\u7b2c\u4e00\u6b21\u795e\u7ecf\u7f51\u7edc\u7814\u7a76\u6d6a\u6f6e\u79f0\u4e3a\u63a7\u5236\u8bba\u3002 \\[ f(x,w) = x_1w_1 + ... + x_nw_n \\] \\(n\\) \u7ec4\u8f93\u5165 \\(x_1, ..., x_n\\) \uff0c\u8f93\u51fa \\(y\\) \uff0c\u6743\u91cd \\(w_1, ..., w_n\\) \uff0c\u901a\u8fc7\u68c0\u9a8c\u51fd\u6570 \\(f(x,w)\\) \u7684\u6b63\u8d1f\u6765\u8bc6\u522b\u4e24\u79cd\u4e0d\u540c\u7c7b\u522b\u7684\u8f93\u5165\uff0c\u8fd9\u4e9b\u6743\u91cd\u7531\u64cd\u4f5c\u4eba\u5458\u8bbe\u5b9a\u3002 \u6700\u8457\u540d\u7684\u5c40\u9650\u6027\u662f\u65e0\u6cd5\u5b66\u4e60\u5f02\u6216\u3002","title":"\u7ebf\u6027\u6a21\u578b\uff0820c50s\uff09"},{"location":"deep-learning/#20c80s","text":"\u5206\u5e03\u5f0f\u8868\u793a\uff0c\u7cfb\u7edf\u4e2d\u7684\u6bcf\u4e00\u4e2a\u8f93\u5165\u90fd\u5e94\u8be5\u7531\u591a\u4e2a\u7279\u5f81\u8868\u793a\u3002 \u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\uff0c\u4ee5\u8fdb\u884c\u8bad\u7ec3\u3002","title":"\u8054\u7ed3\u4e3b\u4e49\uff0820c80s\uff09"},{"location":"deep-learning/#20c90s","text":"LSTM\uff0c\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u5efa\u6a21\u7684\u6570\u5b66\u95ee\u9898\u3002","title":"\u957f\u77ed\u671f\u8bb0\u5fc6\uff0820c90s\uff09"},{"location":"deep-learning/#2006","text":"Hinton et al., 2006a","title":"\u8d2a\u5a6a\u9010\u5c42\u9884\u8bad\u7ec3\uff082006\uff09"},{"location":"model/","text":"\u6a21\u578b \u00b6 \u524d\u5411\u4f20\u64ad\u7528\u4e8e\u9884\u6d4b\uff0c\u53cd\u5411\u4f20\u64ad\u7528\u4e8e\u8bad\u7ec3\u3002 \u5b9a\u4e49\u6a21\u578b \u00b6 import os import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets , transforms device = \"cuda\" if torch . cuda . is_available () else \"cpu\" print ( f \"Using { device } device\" ) \u5b9a\u4e49\u6a21\u578b\uff0c\u9700\u8981\u7ee7\u627f torch.nn.Module \uff0c\u4e14\u9700\u8981\u5b9e\u73b0\u4ee5\u4e0b\u4e24\u4e2a\u51fd\u6570\uff1a __init__ \uff1a\u521d\u59cb\u5316\u7f51\u7edc\u6a21\u578b\u4e2d\u7684\u5404\u79cd\u5c42\u3002 forward \uff1a\u5bf9\u8f93\u5165\u6570\u636e\u8fdb\u884c\u76f8\u5e94\u7684\u64cd\u4f5c\u3002 class NeuralNetwork ( nn . Module ): def __init__ ( self ): super ( NeuralNetwork , self ) . __init__ () self . flatten = nn . Flatten () self . linear_relu_stack = nn . Sequential ( nn . Linear ( in_features = 28 * 28 , out_features = 512 ), nn . ReLU (), nn . Linear ( in_features = 512 , out_features = 512 ), nn . ReLU (), nn . Linear ( in_features = 512 , out_features = 10 ), ) def forward ( self , x ): x = self . flatten ( x ) logits = self . linear_relu_stack ( x ) return logits model = NeuralNetwork () . to ( device ) print ( model ) \u6211\u4eec\u53ef\u4ee5\u5c06\u8f93\u5165\u6570\u636e\u4f20\u5165\u6a21\u578b\uff0c\u4f1a\u81ea\u52a8\u8c03\u7528 forward \u51fd\u6570\u3002\u6a21\u578b\u4f1a\u8fd4\u56de\u4e00\u4e2a 10 \u7ef4\u5f20\u91cf\uff0c\u5176\u4e2d\u5305\u542b\u6bcf\u4e2a\u7c7b\u7684\u539f\u59cb\u9884\u6d4b\u503c\u3002\u6211\u4eec\u4f7f\u7528 nn.Softmax \u51fd\u6570\u6765\u9884\u6d4b\u7c7b\u522b\u7684\u6982\u7387\u3002 X = torch . rand ( 1 , 28 , 28 , device = device ) logits = model ( X ) # \u8c03\u7528forward\u51fd\u6570 # \u5728\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u5e94\u7528Softmax\u51fd\u6570 pred_probab = nn . Softmax ( dim = 1 )( logits ) # \u6700\u5927\u6982\u7387\u503c\u5bf9\u5e94\u7684\u4e0b\u6807 y_pred = pred_probab . argmax ( 1 ) print ( f \"Predicted class: { y_pred } \" ) \u6a21\u578b\u53c2\u6570 \u00b6 \u4f7f\u7528 parameters() \u6216 named_parameters() \u65b9\u6cd5\u53ef\u4ee5\u67e5\u770b\u6a21\u578b\u7684\u53c2\u6570\u3002 print ( f \"Model structure: { model } \\n\\n \" ) for name , param in model . named_parameters (): print ( f \"Layer: { name } | Size: { param . size () } | Values : { param [: 2 ] } \\n \" ) \u81ea\u52a8\u5fae\u5206 \u00b6 \u5728\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u6700\u5e38\u7528\u7684\u7b97\u6cd5\u662f\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\uff0c\u6a21\u578b\u53c2\u6570\u4f1a\u6839\u636e\u635f\u5931\u51fd\u6570\u56de\u4f20\u7684\u68af\u5ea6\u8fdb\u884c\u8c03\u6574\u3002\u4e3a\u4e86\u8ba1\u7b97\u8fd9\u4e9b\u68af\u5ea6\uff0cPyTorch \u6709\u4e00\u4e2a\u5185\u7f6e\u7684\u5fae\u5206\u5f15\u64ce\uff0c\u79f0\u4e3a torch.autograd \uff0c\u5b83\u652f\u6301\u4efb\u4f55\u8ba1\u7b97\u56fe\u7684\u68af\u5ea6\u81ea\u52a8\u8ba1\u7b97\u3002 \u4e0b\u9762\u5b9a\u4e49\u4e86\u6700\u7b80\u5355\u7684\u4e00\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u5177\u6709\u8f93\u5165 x \u3001\u53c2\u6570 w \u548c b \u4ee5\u53ca\u4e00\u4e9b\u635f\u5931\u51fd\u6570\u3002 import torch x = torch . ones ( 5 ) # \u8f93\u5165 y = torch . zeros ( 3 ) # \u671f\u5f85\u7684\u8f93\u51fa w = torch . randn ( 5 , 3 , requires_grad = True ) b = torch . randn ( 3 , requires_grad = True ) z = torch . matmul ( x , w ) + b loss = torch . nn . functional . binary_cross_entropy_with_logits ( z , y ) \u5728\u8fd9\u4e2a\u7f51\u7edc\u4e2d\uff0c w \u548c b \u662f\u6211\u4eec\u9700\u8981\u4f18\u5316\u7684\u53c2\u6570\uff0c\u8bbe\u7f6e\u4e86 requires_grad=True \u5c5e\u6027\u3002\uff08\u53ef\u4ee5\u5728\u521b\u5efa\u5f20\u91cf\u65f6\u8bbe\u7f6e\u8be5\u5c5e\u6027\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528 x.requires_grad_(True) \u6765\u8bbe\u7f6e\uff09 \u6784\u5efa\u8ba1\u7b97\u56fe\u7684\u51fd\u6570\u662f Function \u7c7b\u7684\u4e00\u4e2a\u5bf9\u8c61\u3002\u8fd9\u4e2a\u5bf9\u8c61\u77e5\u9053\u5982\u4f55\u8ba1\u7b97\u6b63\u5411\u7684\u51fd\u6570\uff0c\u4ee5\u53ca\u5982\u4f55\u5728\u53cd\u5411\u4f20\u64ad\u6b65\u9aa4\u4e2d\u8ba1\u7b97\u5bfc\u6570\uff0c\u53ef\u4ee5\u901a\u8fc7\u5f20\u91cf\u7684 grad_fn \u5c5e\u6027\u67e5\u770b\u3002 print ( f \"Gradient function for z = { z . grad_fn } \" ) print ( f \"Gradient function for loss = { loss . grad_fn } \" ) \u8ba1\u7b97\u68af\u5ea6 \u00b6 \u4e3a\u4e86\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u4e2d\u53c2\u6570\u7684\u6743\u91cd\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u5bf9\u53c2\u6570\u7684\u5bfc\u6570\u3002\u6211\u4eec\u53ef\u4ee5\u8c03\u7528 loss.backward() \u6765\u5b8c\u6210\u8fd9\u4e00\u64cd\u4f5c\uff0c\u5728 w.grad \u548c b.grad \u4e2d\u53ef\u4ee5\u67e5\u770b\u76f8\u5e94\u7684\u5bfc\u6570\u503c\u3002 loss . backward () print ( w . grad ) print ( b . grad ) \u4e0d\u4f7f\u7528\u68af\u5ea6\u8ddf\u8e2a \u00b6 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6240\u6709\u5f20\u91cf\u7684\u5c5e\u6027\u90fd\u8bbe\u7f6e\u4e3a requires_grad=True \uff0c\u7528\u6765\u8ddf\u8e2a\u5b83\u4eec\u7684\u8ba1\u7b97\u5386\u53f2\u5e76\u652f\u6301\u68af\u5ea6\u8ba1\u7b97\u3002\u4f46\u662f\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6211\u4eec\u4e0d\u9700\u8981\u8fd9\u6837\u505a\uff08\u5982\u679c\u8fd9\u6837\u505a\u5c31\u4f1a\u5f71\u54cd\u6548\u7387\uff09\uff0c\u4f8b\u5982\uff0c \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\u540e\u5c06\u5176\u7528\u4e8e\u9884\u6d4b\u65f6 \uff0c\u53ea\u9700\u8981\u524d\u5411\u8ba1\u7b97\u5373\u53ef\u3002\u5177\u4f53\u64cd\u4f5c\u5982\u4e0b\uff1a z = torch . matmul ( x , w ) + b print ( z . requires_grad ) with torch . no_grad (): z = torch . matmul ( x , w ) + b print ( z . requires_grad ) \u6216\u8005\u4f7f\u7528 detach() \u65b9\u6cd5\uff1a z = torch . matmul ( x , w ) + b z_det = z . detach () print ( z_det . requires_grad ) \u4f18\u5316\u6a21\u578b\u53c2\u6570 \u00b6 \u8bad\u7ec3\u6a21\u578b\u662f\u4e00\u4e2a\u8fed\u4ee3\u8fc7\u7a0b\uff1b\u5728\u6bcf\u6b21\u8fed\u4ee3\uff08epoch\uff09\u4e2d\uff0c\u6a21\u578b\u5bf9\u8f93\u51fa\u8fdb\u884c\u9884\u6d4b\uff0c\u9996\u5148\u8ba1\u7b97\u731c\u6d4b\u503c\u4e0e\u771f\u5b9e\u503c\u7684\u8bef\u5dee\uff08\u635f\u5931\uff09\uff0c\u7136\u540e\u8ba1\u7b97\u8bef\u5dee\u5173\u4e8e\u5176\u53c2\u6570\u7684\u5bfc\u6570\uff0c\u6700\u540e\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u4f18\u5316\u8fd9\u4e9b\u53c2\u6570\u3002 import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets from torchvision.transforms import ToTensor , Lambda training_data = datasets . FashionMNIST ( root = \"data\" , train = True , download = True , transform = ToTensor () ) test_data = datasets . FashionMNIST ( root = \"data\" , train = False , download = True , transform = ToTensor () ) train_dataloader = DataLoader ( training_data , batch_size = 64 ) test_dataloader = DataLoader ( test_data , batch_size = 64 ) class NeuralNetwork ( nn . Module ): def __init__ ( self ): super ( NeuralNetwork , self ) . __init__ () self . flatten = nn . Flatten () self . linear_relu_stack = nn . Sequential ( nn . Linear ( 28 * 28 , 512 ), nn . ReLU (), nn . Linear ( 512 , 512 ), nn . ReLU (), nn . Linear ( 512 , 10 ), ) def forward ( self , x ): x = self . flatten ( x ) logits = self . linear_relu_stack ( x ) return logits model = NeuralNetwork () \u8d85\u53c2\u6570 \u00b6 \u8d85\u53c2\u6570\u662f\u53ef\u8c03\u6574\u7684\u53c2\u6570\uff0c\u4e0d\u540c\u7684\u8d85\u53c2\u6570\u503c\u4f1a\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u548c\u6536\u655b\u901f\u5ea6\u3002 \u8fd9\u6b21\u8bad\u7ec3\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4ee5\u4e0b\u8d85\u53c2\u6570\uff1a \u8bad\u7ec3\u6b21\u6570 epochs \uff1a\u8fed\u4ee3\u6570\u636e\u96c6\u7684\u6b21\u6570\u3002 \u6279\u5904\u7406\u5927\u5c0f batch_size \uff1a\u6bcf\u6b21\u4f20\u5165\u7f51\u7edc\u4e2d\u7684\u6837\u672c\u6570\u91cf\u3002 \u5b66\u4e60\u7387 learning_rate \uff1a\u5728\u6bcf\u4e2a\u6279\u6b21\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u7684\u7a0b\u5ea6\u3002\u8f83\u5c0f\u7684\u503c\u4f1a\u4ea7\u751f\u8f83\u6162\u7684\u5b66\u4e60\u901f\u5ea6\uff0c\u800c\u8f83\u5927\u7684\u503c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u671f\u95f4\u51fa\u73b0\u4e0d\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\u3002 learning_rate = 1e-3 batch_size = 64 epochs = 5 \u4f18\u5316\u5faa\u73af \u00b6 \u8bbe\u7f6e\u597d\u8d85\u53c2\u6570\u540e\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u4f7f\u7528\u4f18\u5316\u5faa\u73af\u6765\u8bad\u7ec3\u548c\u4f18\u5316\u6211\u4eec\u7684\u6a21\u578b\u3002 \u6bcf\u4e2aepoch\u5305\u62ec\u4ee5\u4e0b\u4e24\u4e2a\u5faa\u73af\uff1a \u8bad\u7ec3\u5faa\u73af\uff1a\u8fed\u4ee3\u8bad\u7ec3\u6570\u636e\u96c6\u5e76\u5c1d\u8bd5\u6536\u655b\u5230\u6700\u4f73\u53c2\u6570\u3002 \u9a8c\u8bc1/\u6d4b\u8bd5\u5faa\u73af\uff1a\u8fed\u4ee3\u6d4b\u8bd5\u6570\u636e\u96c6\u4ee5\u68c0\u67e5\u6a21\u578b\u6027\u80fd\u662f\u5426\u6b63\u5728\u6539\u5584\u3002 \u635f\u5931\u51fd\u6570 \u00b6 \u635f\u5931\u51fd\u6570\u7528\u6765\u8861\u91cf\u6a21\u578b\u9884\u6d4b\u5f97\u5230\u7684\u7ed3\u679c\u4e0e\u771f\u5b9e\u503c\u7684\u5dee\u5f02\u7a0b\u5ea6\uff0c\u635f\u5931\u503c\u8d8a\u5c0f\u8d8a\u597d\u3002 \u5e38\u89c1\u7684\u635f\u5931\u51fd\u6570\u5305\u62ec\u7528\u4e8e\u56de\u5f52\u4efb\u52a1\u7684 nn.MSELoss \uff08\u5747\u65b9\u8bef\u5dee\uff09\u548c\u7528\u4e8e\u5206\u7c7b\u7684 nn.NLLLoss \uff08\u8d1f\u5bf9\u6570\u4f3c\u7136\uff09\u3002 nn.CrossEntropyLoss \u7ed3\u5408 nn.LogSoftmax \u548c nn.NLLLoss \u3002 \u8fd9\u91cc\u6211\u4eec\u5c06\u6a21\u578b\u7684\u8f93\u51fa logits \u4f20\u9012\u7ed9 nn.CrossEntropyLoss \uff0c\u8fdb\u884c\u5f52\u4e00\u5316\u5e76\u8ba1\u7b97\u9884\u6d4b\u8bef\u5dee\u3002 # \u521d\u59cb\u5316\u635f\u5931\u51fd\u6570 loss_fn = nn . CrossEntropyLoss () \u4f18\u5316\u5668 \u00b6 \u4f18\u5316\u662f\u5728\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u4e2d\u8c03\u6574\u6a21\u578b\u53c2\u6570\u4ee5\u51cf\u5c11\u6a21\u578b\u8bef\u5dee\u7684\u8fc7\u7a0b\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4f7f\u7528 SGD \u4f18\u5316\u5668\uff1b torch.optim \u4e2d\u63d0\u4f9b\u4e86\u5f88\u591a\u4f18\u5316\u5668\uff0c \u4f8b\u5982 ADAM \u548c RMSProp \u3002 # \u4f20\u5165\u9700\u8981\u4f18\u5316\u7684\u53c2\u6570\u548c\u5b66\u4e60\u7387 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate ) \u5b9e\u8df5 \u00b6 \u5728\u8bad\u7ec3\u5faa\u73af\u4e2d\uff0c\u4f18\u5316\u5206\u4e09\u4e2a\u6b65\u9aa4\u8fdb\u884c\uff1a \u8c03\u7528 optimizer.zero_grad() \u5c06\u6a21\u578b\u53c2\u6570\u7684\u68af\u5ea6\u5f52\u96f6\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u68af\u5ea6\u4f1a\u7d2f\u52a0\u3002 \u8c03\u7528 loss.backward() \u6765\u53cd\u5411\u4f20\u64ad\u9884\u6d4b\u635f\u5931\u3002PyTorch \u5b58\u50a8\u6bcf\u4e2a\u53c2\u6570\u7684\u635f\u5931\u68af\u5ea6\u3002 \u8ba1\u7b97\u68af\u5ea6\u5b8c\u6210\u540e\uff0c\u8c03\u7528 optimizer.step() \u6765\u8c03\u6574\u53c2\u6570\u3002 # \u4f18\u5316\u6a21\u578b\u53c2\u6570 def train_loop ( dataloader , model , loss_fn , optimizer , device ): size = len ( dataloader . dataset ) for batch , ( X , y ) in enumerate ( dataloader ): X = X . to ( device ) y = y . to ( device ) # \u524d\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u9884\u6d4b\u503c pred = model ( X ) # \u8ba1\u7b97\u635f\u5931 loss = loss_fn ( pred , y ) # \u53cd\u5411\u4f20\u64ad\uff0c\u4f18\u5316\u53c2\u6570 optimizer . zero_grad () # \u68af\u5ea6\u5f52\u96f6 loss . backward () # \u53cd\u5411\u4f20\u64ad\u9884\u6d4b\u635f\u5931 optimizer . step () # \u8c03\u6574\u53c2\u6570 if batch % 100 == 0 : loss , current = loss . item (), batch * len ( X ) print ( f \"loss: { loss : >7f } [ { current : >5d } / { size : >5d } ]\" ) # \u6d4b\u8bd5\u6a21\u578b\u6027\u80fd def test_loop ( dataloader , model , loss_fn , device ): size = len ( dataloader . dataset ) num_batches = len ( dataloader ) test_loss , correct = 0 , 0 with torch . no_grad (): for X , y in dataloader : X = X . to ( device ) y = y . to ( device ) # \u524d\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u9884\u6d4b\u503c pred = model ( X ) # \u8ba1\u7b97\u635f\u5931 test_loss += loss_fn ( pred , y ) . item () # \u8ba1\u7b97\u51c6\u786e\u7387 correct += ( pred . argmax ( 1 ) == y ) . type ( torch . float ) . sum () . item () test_loss /= num_batches correct /= size print ( f \"Test Error: \\n Accuracy: { ( 100 * correct ) : >0.1f } %, Avg loss: { test_loss : >8f } \\n \" ) \u6211\u4eec\u521d\u59cb\u5316\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\uff0c\u5e76\u5c06\u5176\u4f20\u9012\u7ed9 train_loop \u548c test_loop \u3002 loss_fn = nn . CrossEntropyLoss () optimizer = torch . optim . SGD ( params = model . parameters (), lr = learning_rate ) epochs = 10 for t in range ( epochs ): print ( f \"Epoch { t + 1 } \\n -------------------------------\" ) train_loop ( train_dataloader , model , loss_fn , optimizer , device ) test_loop ( test_dataloader , model , loss_fn , device ) print ( \"Done!\" ) \u4fdd\u5b58\u548c\u52a0\u8f7d\u6a21\u578b \u00b6 import torch import torchvision.models as models PyTorch \u6a21\u578b\u5c06\u5b66\u4e60\u5230\u7684\u53c2\u6570\u5b58\u50a8\u5728\u5185\u90e8\u72b6\u6001\u5b57\u5178\u4e2d\uff0c\u79f0\u4e3a state_dict \u3002 \u53ef\u4ee5\u901a\u8fc7 torch.save \u65b9\u6cd5\u4fdd\u5b58\uff1a torch.save(model.state_dict(),model_path) \u4fdd\u5b58\u548c\u52a0\u8f7d\u6a21\u578b\uff1a torch . save ( model , 'model.pth' ) model = torch . load ( 'model.pth' ) \u52a0\u8f7d\u6a21\u578b\u5206\u4e3a\u4e24\u6b65\uff1a \u5148\u52a0\u8f7d\u6a21\u578b\u4e2d\u7684 state_dict \u53c2\u6570\uff0c state_dict=torch.load(model_path) \u7136\u540e\u52a0\u8f7d state_dict \u5230\u5b9a\u4e49\u597d\u7684\u6a21\u578b\u4e2d\uff0c model.load_state_dict(state_dict,strict=True/False) \uff0c strict \u8868\u793a\u662f\u5426\u4e25\u683c\u52a0\u8f7d\u6a21\u578b\u53c2\u6570\uff0c load_state_dict() \u4f1a\u8fd4\u56de missing_keys \u548c unexpected_keys \u4e24\u4e2a\u53c2\u6570 # \u6837\u4f8b\u4ee3\u7801\u5982\u4e0b model = models . vgg16 ( pretrained = True ) # pretrained=True\u52a0\u8f7d\u9884\u8bad\u7ec3\u597d\u7684\u53c2\u6570 torch . save ( model . state_dict (), 'model_weights.pth' ) # \u8981\u52a0\u8f7d\u6a21\u578b\u6743\u91cd\uff0c\u9996\u5148\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u76f8\u540c\u6a21\u578b\u7684\u5b9e\u4f8b\uff0c\u7136\u540e\u4f7f\u7528load_state_dict()\u65b9\u6cd5\u52a0\u8f7d\u53c2\u6570\u3002 model = models . vgg16 () # \u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u597d\u7684\u53c2\u6570 model . load_state_dict ( torch . load ( 'model_weights.pth' )) model . eval () # \u5c06\u6a21\u578b\u8bbe\u7f6e\u4e3a\u6d4b\u8bd5\u6a21\u5f0f\uff0c\u907f\u514ddropout\u548cbatch normalization\u5bf9\u9884\u6d4b\u7ed3\u679c\u9020\u6210\u7684\u5f71\u54cd \u9644\u5b8c\u6574\u4ee3\u7801 \u00b6 import os import matplotlib.pyplot as plt from torchvision.transforms import ToTensor import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets , transforms # \u8bad\u7ec3\u6570\u636e\u96c6 training_data = datasets . FashionMNIST ( root = \"data\" , train = True , download = True , transform = ToTensor () # \u5bf9\u6837\u672c\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u8f6c\u6362\u4e3a\u5f20\u91cf\u6570\u636e ) # \u6d4b\u8bd5\u6570\u636e\u96c6 test_data = datasets . FashionMNIST ( root = \"data\" , train = False , download = True , transform = ToTensor () # \u5bf9\u6837\u672c\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u8f6c\u6362\u4e3a\u5f20\u91cf\u6570\u636e ) # \u6807\u7b7e\u5b57\u5178\uff0c\u4e00\u4e2akey\u952e\u5bf9\u5e94\u4e00\u4e2alabel labels_map = { 0 : \"T-Shirt\" , 1 : \"Trouser\" , 2 : \"Pullover\" , 3 : \"Dress\" , 4 : \"Coat\" , 5 : \"Sandal\" , 6 : \"Shirt\" , 7 : \"Sneaker\" , 8 : \"Bag\" , 9 : \"Ankle Boot\" , } # \u8bbe\u7f6e\u753b\u5e03\u5927\u5c0f # figure = plt.figure(figsize=(8, 8)) # cols, rows = 3, 3 # for i in range(1, cols * rows + 1): # # \u968f\u673a\u751f\u6210\u4e00\u4e2a\u7d22\u5f15 # sample_idx = torch.randint(len(training_data), size=(1,)).item() # # \u83b7\u53d6\u6837\u672c\u53ca\u5176\u5bf9\u5e94\u7684\u6807\u7b7e # img, label = training_data[sample_idx] # figure.add_subplot(rows, cols, i) # # \u8bbe\u7f6e\u6807\u9898 # plt.title(labels_map[label]) # # \u4e0d\u663e\u793a\u5750\u6807\u8f74 # plt.axis(\"off\") # # \u663e\u793a\u7070\u5ea6\u56fe # plt.imshow(img.squeeze(), cmap=\"gray\") # plt.show() # \u8bad\u7ec3\u6570\u636e\u52a0\u8f7d\u5668 train_dataloader = DataLoader ( dataset = training_data , # \u8bbe\u7f6e\u6279\u91cf\u5927\u5c0f batch_size = 64 , # \u6253\u4e71\u6837\u672c\u7684\u987a\u5e8f shuffle = True ) # \u6d4b\u8bd5\u6570\u636e\u52a0\u8f7d\u5668 test_dataloader = DataLoader ( dataset = test_data , batch_size = 64 , shuffle = True ) # \u5c55\u793a\u56fe\u7247\u548c\u6807\u7b7e # train_features, train_labels = next(iter(train_dataloader)) # print(f\"Feature batch shape: {train_features.size()}\") # print(f\"Labels batch shape: {train_labels.size()}\") # img = train_features[0].squeeze() # label = train_labels[0] # plt.imshow(img, cmap=\"gray\") # plt.show() # print(f\"Label: {label}\") # \u6a21\u578b\u5b9a\u4e49 class NeuralNetwork ( nn . Module ): def __init__ ( self ): super ( NeuralNetwork , self ) . __init__ () self . flatten = nn . Flatten () self . linear_relu_stack = nn . Sequential ( nn . Linear ( in_features = 28 * 28 , out_features = 512 ), nn . ReLU (), nn . Linear ( in_features = 512 , out_features = 512 ), nn . ReLU (), nn . Linear ( in_features = 512 , out_features = 10 ), ) def forward ( self , x ): x = self . flatten ( x ) logits = self . linear_relu_stack ( x ) return logits # \u4f18\u5316\u6a21\u578b\u53c2\u6570 def train_loop ( dataloader , model , loss_fn , optimizer , device ): size = len ( dataloader . dataset ) for batch , ( X , y ) in enumerate ( dataloader ): X = X . to ( device ) y = y . to ( device ) # \u524d\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u9884\u6d4b\u503c pred = model ( X ) # \u8ba1\u7b97\u635f\u5931 loss = loss_fn ( pred , y ) # \u53cd\u5411\u4f20\u64ad\uff0c\u4f18\u5316\u53c2\u6570 optimizer . zero_grad () loss . backward () optimizer . step () if batch % 100 == 0 : loss , current = loss . item (), batch * len ( X ) print ( f \"loss: { loss : >7f } [ { current : >5d } / { size : >5d } ]\" ) # \u6d4b\u8bd5\u6a21\u578b\u6027\u80fd def test_loop ( dataloader , model , loss_fn , device ): size = len ( dataloader . dataset ) num_batches = len ( dataloader ) test_loss , correct = 0 , 0 with torch . no_grad (): for X , y in dataloader : X = X . to ( device ) y = y . to ( device ) # \u524d\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u9884\u6d4b\u503c pred = model ( X ) # \u8ba1\u7b97\u635f\u5931 test_loss += loss_fn ( pred , y ) . item () # \u8ba1\u7b97\u51c6\u786e\u7387 correct += ( pred . argmax ( 1 ) == y ) . type ( torch . float ) . sum () . item () test_loss /= num_batches correct /= size print ( f \"Test Error: \\n Accuracy: { ( 100 * correct ) : >0.1f } %, Avg loss: { test_loss : >8f } \\n \" ) if __name__ == '__main__' : device = \"cuda\" if torch . cuda . is_available () else \"cpu\" print ( f \"Using { device } device\" ) # \u5b9a\u4e49\u6a21\u578b model = NeuralNetwork () . to ( device ) # \u8bbe\u7f6e\u8d85\u53c2\u6570 learning_rate = 1e-3 batch_size = 64 epochs = 5 # \u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668 loss_fn = nn . CrossEntropyLoss () optimizer = torch . optim . SGD ( params = model . parameters (), lr = learning_rate ) # \u8bad\u7ec3\u6a21\u578b for t in range ( epochs ): print ( f \"Epoch { t + 1 } \\n -------------------------------\" ) train_loop ( train_dataloader , model , loss_fn , optimizer , device ) test_loop ( test_dataloader , model , loss_fn , device ) print ( \"Done!\" ) # \u4fdd\u5b58\u6a21\u578b torch . save ( model . state_dict (), 'model_weights.pth' )","title":"\u6a21\u578b"},{"location":"model/#_1","text":"\u524d\u5411\u4f20\u64ad\u7528\u4e8e\u9884\u6d4b\uff0c\u53cd\u5411\u4f20\u64ad\u7528\u4e8e\u8bad\u7ec3\u3002","title":"\u6a21\u578b"},{"location":"model/#_2","text":"import os import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets , transforms device = \"cuda\" if torch . cuda . is_available () else \"cpu\" print ( f \"Using { device } device\" ) \u5b9a\u4e49\u6a21\u578b\uff0c\u9700\u8981\u7ee7\u627f torch.nn.Module \uff0c\u4e14\u9700\u8981\u5b9e\u73b0\u4ee5\u4e0b\u4e24\u4e2a\u51fd\u6570\uff1a __init__ \uff1a\u521d\u59cb\u5316\u7f51\u7edc\u6a21\u578b\u4e2d\u7684\u5404\u79cd\u5c42\u3002 forward \uff1a\u5bf9\u8f93\u5165\u6570\u636e\u8fdb\u884c\u76f8\u5e94\u7684\u64cd\u4f5c\u3002 class NeuralNetwork ( nn . Module ): def __init__ ( self ): super ( NeuralNetwork , self ) . __init__ () self . flatten = nn . Flatten () self . linear_relu_stack = nn . Sequential ( nn . Linear ( in_features = 28 * 28 , out_features = 512 ), nn . ReLU (), nn . Linear ( in_features = 512 , out_features = 512 ), nn . ReLU (), nn . Linear ( in_features = 512 , out_features = 10 ), ) def forward ( self , x ): x = self . flatten ( x ) logits = self . linear_relu_stack ( x ) return logits model = NeuralNetwork () . to ( device ) print ( model ) \u6211\u4eec\u53ef\u4ee5\u5c06\u8f93\u5165\u6570\u636e\u4f20\u5165\u6a21\u578b\uff0c\u4f1a\u81ea\u52a8\u8c03\u7528 forward \u51fd\u6570\u3002\u6a21\u578b\u4f1a\u8fd4\u56de\u4e00\u4e2a 10 \u7ef4\u5f20\u91cf\uff0c\u5176\u4e2d\u5305\u542b\u6bcf\u4e2a\u7c7b\u7684\u539f\u59cb\u9884\u6d4b\u503c\u3002\u6211\u4eec\u4f7f\u7528 nn.Softmax \u51fd\u6570\u6765\u9884\u6d4b\u7c7b\u522b\u7684\u6982\u7387\u3002 X = torch . rand ( 1 , 28 , 28 , device = device ) logits = model ( X ) # \u8c03\u7528forward\u51fd\u6570 # \u5728\u7b2c\u4e00\u4e2a\u7ef4\u5ea6\u5e94\u7528Softmax\u51fd\u6570 pred_probab = nn . Softmax ( dim = 1 )( logits ) # \u6700\u5927\u6982\u7387\u503c\u5bf9\u5e94\u7684\u4e0b\u6807 y_pred = pred_probab . argmax ( 1 ) print ( f \"Predicted class: { y_pred } \" )","title":"\u5b9a\u4e49\u6a21\u578b"},{"location":"model/#_3","text":"\u4f7f\u7528 parameters() \u6216 named_parameters() \u65b9\u6cd5\u53ef\u4ee5\u67e5\u770b\u6a21\u578b\u7684\u53c2\u6570\u3002 print ( f \"Model structure: { model } \\n\\n \" ) for name , param in model . named_parameters (): print ( f \"Layer: { name } | Size: { param . size () } | Values : { param [: 2 ] } \\n \" )","title":"\u6a21\u578b\u53c2\u6570"},{"location":"model/#_4","text":"\u5728\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u6700\u5e38\u7528\u7684\u7b97\u6cd5\u662f\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\uff0c\u6a21\u578b\u53c2\u6570\u4f1a\u6839\u636e\u635f\u5931\u51fd\u6570\u56de\u4f20\u7684\u68af\u5ea6\u8fdb\u884c\u8c03\u6574\u3002\u4e3a\u4e86\u8ba1\u7b97\u8fd9\u4e9b\u68af\u5ea6\uff0cPyTorch \u6709\u4e00\u4e2a\u5185\u7f6e\u7684\u5fae\u5206\u5f15\u64ce\uff0c\u79f0\u4e3a torch.autograd \uff0c\u5b83\u652f\u6301\u4efb\u4f55\u8ba1\u7b97\u56fe\u7684\u68af\u5ea6\u81ea\u52a8\u8ba1\u7b97\u3002 \u4e0b\u9762\u5b9a\u4e49\u4e86\u6700\u7b80\u5355\u7684\u4e00\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u5177\u6709\u8f93\u5165 x \u3001\u53c2\u6570 w \u548c b \u4ee5\u53ca\u4e00\u4e9b\u635f\u5931\u51fd\u6570\u3002 import torch x = torch . ones ( 5 ) # \u8f93\u5165 y = torch . zeros ( 3 ) # \u671f\u5f85\u7684\u8f93\u51fa w = torch . randn ( 5 , 3 , requires_grad = True ) b = torch . randn ( 3 , requires_grad = True ) z = torch . matmul ( x , w ) + b loss = torch . nn . functional . binary_cross_entropy_with_logits ( z , y ) \u5728\u8fd9\u4e2a\u7f51\u7edc\u4e2d\uff0c w \u548c b \u662f\u6211\u4eec\u9700\u8981\u4f18\u5316\u7684\u53c2\u6570\uff0c\u8bbe\u7f6e\u4e86 requires_grad=True \u5c5e\u6027\u3002\uff08\u53ef\u4ee5\u5728\u521b\u5efa\u5f20\u91cf\u65f6\u8bbe\u7f6e\u8be5\u5c5e\u6027\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528 x.requires_grad_(True) \u6765\u8bbe\u7f6e\uff09 \u6784\u5efa\u8ba1\u7b97\u56fe\u7684\u51fd\u6570\u662f Function \u7c7b\u7684\u4e00\u4e2a\u5bf9\u8c61\u3002\u8fd9\u4e2a\u5bf9\u8c61\u77e5\u9053\u5982\u4f55\u8ba1\u7b97\u6b63\u5411\u7684\u51fd\u6570\uff0c\u4ee5\u53ca\u5982\u4f55\u5728\u53cd\u5411\u4f20\u64ad\u6b65\u9aa4\u4e2d\u8ba1\u7b97\u5bfc\u6570\uff0c\u53ef\u4ee5\u901a\u8fc7\u5f20\u91cf\u7684 grad_fn \u5c5e\u6027\u67e5\u770b\u3002 print ( f \"Gradient function for z = { z . grad_fn } \" ) print ( f \"Gradient function for loss = { loss . grad_fn } \" )","title":"\u81ea\u52a8\u5fae\u5206"},{"location":"model/#_5","text":"\u4e3a\u4e86\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u4e2d\u53c2\u6570\u7684\u6743\u91cd\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u5bf9\u53c2\u6570\u7684\u5bfc\u6570\u3002\u6211\u4eec\u53ef\u4ee5\u8c03\u7528 loss.backward() \u6765\u5b8c\u6210\u8fd9\u4e00\u64cd\u4f5c\uff0c\u5728 w.grad \u548c b.grad \u4e2d\u53ef\u4ee5\u67e5\u770b\u76f8\u5e94\u7684\u5bfc\u6570\u503c\u3002 loss . backward () print ( w . grad ) print ( b . grad )","title":"\u8ba1\u7b97\u68af\u5ea6"},{"location":"model/#_6","text":"\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6240\u6709\u5f20\u91cf\u7684\u5c5e\u6027\u90fd\u8bbe\u7f6e\u4e3a requires_grad=True \uff0c\u7528\u6765\u8ddf\u8e2a\u5b83\u4eec\u7684\u8ba1\u7b97\u5386\u53f2\u5e76\u652f\u6301\u68af\u5ea6\u8ba1\u7b97\u3002\u4f46\u662f\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6211\u4eec\u4e0d\u9700\u8981\u8fd9\u6837\u505a\uff08\u5982\u679c\u8fd9\u6837\u505a\u5c31\u4f1a\u5f71\u54cd\u6548\u7387\uff09\uff0c\u4f8b\u5982\uff0c \u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\u540e\u5c06\u5176\u7528\u4e8e\u9884\u6d4b\u65f6 \uff0c\u53ea\u9700\u8981\u524d\u5411\u8ba1\u7b97\u5373\u53ef\u3002\u5177\u4f53\u64cd\u4f5c\u5982\u4e0b\uff1a z = torch . matmul ( x , w ) + b print ( z . requires_grad ) with torch . no_grad (): z = torch . matmul ( x , w ) + b print ( z . requires_grad ) \u6216\u8005\u4f7f\u7528 detach() \u65b9\u6cd5\uff1a z = torch . matmul ( x , w ) + b z_det = z . detach () print ( z_det . requires_grad )","title":"\u4e0d\u4f7f\u7528\u68af\u5ea6\u8ddf\u8e2a"},{"location":"model/#_7","text":"\u8bad\u7ec3\u6a21\u578b\u662f\u4e00\u4e2a\u8fed\u4ee3\u8fc7\u7a0b\uff1b\u5728\u6bcf\u6b21\u8fed\u4ee3\uff08epoch\uff09\u4e2d\uff0c\u6a21\u578b\u5bf9\u8f93\u51fa\u8fdb\u884c\u9884\u6d4b\uff0c\u9996\u5148\u8ba1\u7b97\u731c\u6d4b\u503c\u4e0e\u771f\u5b9e\u503c\u7684\u8bef\u5dee\uff08\u635f\u5931\uff09\uff0c\u7136\u540e\u8ba1\u7b97\u8bef\u5dee\u5173\u4e8e\u5176\u53c2\u6570\u7684\u5bfc\u6570\uff0c\u6700\u540e\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u4f18\u5316\u8fd9\u4e9b\u53c2\u6570\u3002 import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets from torchvision.transforms import ToTensor , Lambda training_data = datasets . FashionMNIST ( root = \"data\" , train = True , download = True , transform = ToTensor () ) test_data = datasets . FashionMNIST ( root = \"data\" , train = False , download = True , transform = ToTensor () ) train_dataloader = DataLoader ( training_data , batch_size = 64 ) test_dataloader = DataLoader ( test_data , batch_size = 64 ) class NeuralNetwork ( nn . Module ): def __init__ ( self ): super ( NeuralNetwork , self ) . __init__ () self . flatten = nn . Flatten () self . linear_relu_stack = nn . Sequential ( nn . Linear ( 28 * 28 , 512 ), nn . ReLU (), nn . Linear ( 512 , 512 ), nn . ReLU (), nn . Linear ( 512 , 10 ), ) def forward ( self , x ): x = self . flatten ( x ) logits = self . linear_relu_stack ( x ) return logits model = NeuralNetwork ()","title":"\u4f18\u5316\u6a21\u578b\u53c2\u6570"},{"location":"model/#_8","text":"\u8d85\u53c2\u6570\u662f\u53ef\u8c03\u6574\u7684\u53c2\u6570\uff0c\u4e0d\u540c\u7684\u8d85\u53c2\u6570\u503c\u4f1a\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u548c\u6536\u655b\u901f\u5ea6\u3002 \u8fd9\u6b21\u8bad\u7ec3\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4ee5\u4e0b\u8d85\u53c2\u6570\uff1a \u8bad\u7ec3\u6b21\u6570 epochs \uff1a\u8fed\u4ee3\u6570\u636e\u96c6\u7684\u6b21\u6570\u3002 \u6279\u5904\u7406\u5927\u5c0f batch_size \uff1a\u6bcf\u6b21\u4f20\u5165\u7f51\u7edc\u4e2d\u7684\u6837\u672c\u6570\u91cf\u3002 \u5b66\u4e60\u7387 learning_rate \uff1a\u5728\u6bcf\u4e2a\u6279\u6b21\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u7684\u7a0b\u5ea6\u3002\u8f83\u5c0f\u7684\u503c\u4f1a\u4ea7\u751f\u8f83\u6162\u7684\u5b66\u4e60\u901f\u5ea6\uff0c\u800c\u8f83\u5927\u7684\u503c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u671f\u95f4\u51fa\u73b0\u4e0d\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\u3002 learning_rate = 1e-3 batch_size = 64 epochs = 5","title":"\u8d85\u53c2\u6570"},{"location":"model/#_9","text":"\u8bbe\u7f6e\u597d\u8d85\u53c2\u6570\u540e\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u4f7f\u7528\u4f18\u5316\u5faa\u73af\u6765\u8bad\u7ec3\u548c\u4f18\u5316\u6211\u4eec\u7684\u6a21\u578b\u3002 \u6bcf\u4e2aepoch\u5305\u62ec\u4ee5\u4e0b\u4e24\u4e2a\u5faa\u73af\uff1a \u8bad\u7ec3\u5faa\u73af\uff1a\u8fed\u4ee3\u8bad\u7ec3\u6570\u636e\u96c6\u5e76\u5c1d\u8bd5\u6536\u655b\u5230\u6700\u4f73\u53c2\u6570\u3002 \u9a8c\u8bc1/\u6d4b\u8bd5\u5faa\u73af\uff1a\u8fed\u4ee3\u6d4b\u8bd5\u6570\u636e\u96c6\u4ee5\u68c0\u67e5\u6a21\u578b\u6027\u80fd\u662f\u5426\u6b63\u5728\u6539\u5584\u3002","title":"\u4f18\u5316\u5faa\u73af"},{"location":"model/#_10","text":"\u635f\u5931\u51fd\u6570\u7528\u6765\u8861\u91cf\u6a21\u578b\u9884\u6d4b\u5f97\u5230\u7684\u7ed3\u679c\u4e0e\u771f\u5b9e\u503c\u7684\u5dee\u5f02\u7a0b\u5ea6\uff0c\u635f\u5931\u503c\u8d8a\u5c0f\u8d8a\u597d\u3002 \u5e38\u89c1\u7684\u635f\u5931\u51fd\u6570\u5305\u62ec\u7528\u4e8e\u56de\u5f52\u4efb\u52a1\u7684 nn.MSELoss \uff08\u5747\u65b9\u8bef\u5dee\uff09\u548c\u7528\u4e8e\u5206\u7c7b\u7684 nn.NLLLoss \uff08\u8d1f\u5bf9\u6570\u4f3c\u7136\uff09\u3002 nn.CrossEntropyLoss \u7ed3\u5408 nn.LogSoftmax \u548c nn.NLLLoss \u3002 \u8fd9\u91cc\u6211\u4eec\u5c06\u6a21\u578b\u7684\u8f93\u51fa logits \u4f20\u9012\u7ed9 nn.CrossEntropyLoss \uff0c\u8fdb\u884c\u5f52\u4e00\u5316\u5e76\u8ba1\u7b97\u9884\u6d4b\u8bef\u5dee\u3002 # \u521d\u59cb\u5316\u635f\u5931\u51fd\u6570 loss_fn = nn . CrossEntropyLoss ()","title":"\u635f\u5931\u51fd\u6570"},{"location":"model/#_11","text":"\u4f18\u5316\u662f\u5728\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u4e2d\u8c03\u6574\u6a21\u578b\u53c2\u6570\u4ee5\u51cf\u5c11\u6a21\u578b\u8bef\u5dee\u7684\u8fc7\u7a0b\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4f7f\u7528 SGD \u4f18\u5316\u5668\uff1b torch.optim \u4e2d\u63d0\u4f9b\u4e86\u5f88\u591a\u4f18\u5316\u5668\uff0c \u4f8b\u5982 ADAM \u548c RMSProp \u3002 # \u4f20\u5165\u9700\u8981\u4f18\u5316\u7684\u53c2\u6570\u548c\u5b66\u4e60\u7387 optimizer = torch . optim . SGD ( model . parameters (), lr = learning_rate )","title":"\u4f18\u5316\u5668"},{"location":"model/#_12","text":"\u5728\u8bad\u7ec3\u5faa\u73af\u4e2d\uff0c\u4f18\u5316\u5206\u4e09\u4e2a\u6b65\u9aa4\u8fdb\u884c\uff1a \u8c03\u7528 optimizer.zero_grad() \u5c06\u6a21\u578b\u53c2\u6570\u7684\u68af\u5ea6\u5f52\u96f6\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u68af\u5ea6\u4f1a\u7d2f\u52a0\u3002 \u8c03\u7528 loss.backward() \u6765\u53cd\u5411\u4f20\u64ad\u9884\u6d4b\u635f\u5931\u3002PyTorch \u5b58\u50a8\u6bcf\u4e2a\u53c2\u6570\u7684\u635f\u5931\u68af\u5ea6\u3002 \u8ba1\u7b97\u68af\u5ea6\u5b8c\u6210\u540e\uff0c\u8c03\u7528 optimizer.step() \u6765\u8c03\u6574\u53c2\u6570\u3002 # \u4f18\u5316\u6a21\u578b\u53c2\u6570 def train_loop ( dataloader , model , loss_fn , optimizer , device ): size = len ( dataloader . dataset ) for batch , ( X , y ) in enumerate ( dataloader ): X = X . to ( device ) y = y . to ( device ) # \u524d\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u9884\u6d4b\u503c pred = model ( X ) # \u8ba1\u7b97\u635f\u5931 loss = loss_fn ( pred , y ) # \u53cd\u5411\u4f20\u64ad\uff0c\u4f18\u5316\u53c2\u6570 optimizer . zero_grad () # \u68af\u5ea6\u5f52\u96f6 loss . backward () # \u53cd\u5411\u4f20\u64ad\u9884\u6d4b\u635f\u5931 optimizer . step () # \u8c03\u6574\u53c2\u6570 if batch % 100 == 0 : loss , current = loss . item (), batch * len ( X ) print ( f \"loss: { loss : >7f } [ { current : >5d } / { size : >5d } ]\" ) # \u6d4b\u8bd5\u6a21\u578b\u6027\u80fd def test_loop ( dataloader , model , loss_fn , device ): size = len ( dataloader . dataset ) num_batches = len ( dataloader ) test_loss , correct = 0 , 0 with torch . no_grad (): for X , y in dataloader : X = X . to ( device ) y = y . to ( device ) # \u524d\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u9884\u6d4b\u503c pred = model ( X ) # \u8ba1\u7b97\u635f\u5931 test_loss += loss_fn ( pred , y ) . item () # \u8ba1\u7b97\u51c6\u786e\u7387 correct += ( pred . argmax ( 1 ) == y ) . type ( torch . float ) . sum () . item () test_loss /= num_batches correct /= size print ( f \"Test Error: \\n Accuracy: { ( 100 * correct ) : >0.1f } %, Avg loss: { test_loss : >8f } \\n \" ) \u6211\u4eec\u521d\u59cb\u5316\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\uff0c\u5e76\u5c06\u5176\u4f20\u9012\u7ed9 train_loop \u548c test_loop \u3002 loss_fn = nn . CrossEntropyLoss () optimizer = torch . optim . SGD ( params = model . parameters (), lr = learning_rate ) epochs = 10 for t in range ( epochs ): print ( f \"Epoch { t + 1 } \\n -------------------------------\" ) train_loop ( train_dataloader , model , loss_fn , optimizer , device ) test_loop ( test_dataloader , model , loss_fn , device ) print ( \"Done!\" )","title":"\u5b9e\u8df5"},{"location":"model/#_13","text":"import torch import torchvision.models as models PyTorch \u6a21\u578b\u5c06\u5b66\u4e60\u5230\u7684\u53c2\u6570\u5b58\u50a8\u5728\u5185\u90e8\u72b6\u6001\u5b57\u5178\u4e2d\uff0c\u79f0\u4e3a state_dict \u3002 \u53ef\u4ee5\u901a\u8fc7 torch.save \u65b9\u6cd5\u4fdd\u5b58\uff1a torch.save(model.state_dict(),model_path) \u4fdd\u5b58\u548c\u52a0\u8f7d\u6a21\u578b\uff1a torch . save ( model , 'model.pth' ) model = torch . load ( 'model.pth' ) \u52a0\u8f7d\u6a21\u578b\u5206\u4e3a\u4e24\u6b65\uff1a \u5148\u52a0\u8f7d\u6a21\u578b\u4e2d\u7684 state_dict \u53c2\u6570\uff0c state_dict=torch.load(model_path) \u7136\u540e\u52a0\u8f7d state_dict \u5230\u5b9a\u4e49\u597d\u7684\u6a21\u578b\u4e2d\uff0c model.load_state_dict(state_dict,strict=True/False) \uff0c strict \u8868\u793a\u662f\u5426\u4e25\u683c\u52a0\u8f7d\u6a21\u578b\u53c2\u6570\uff0c load_state_dict() \u4f1a\u8fd4\u56de missing_keys \u548c unexpected_keys \u4e24\u4e2a\u53c2\u6570 # \u6837\u4f8b\u4ee3\u7801\u5982\u4e0b model = models . vgg16 ( pretrained = True ) # pretrained=True\u52a0\u8f7d\u9884\u8bad\u7ec3\u597d\u7684\u53c2\u6570 torch . save ( model . state_dict (), 'model_weights.pth' ) # \u8981\u52a0\u8f7d\u6a21\u578b\u6743\u91cd\uff0c\u9996\u5148\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u76f8\u540c\u6a21\u578b\u7684\u5b9e\u4f8b\uff0c\u7136\u540e\u4f7f\u7528load_state_dict()\u65b9\u6cd5\u52a0\u8f7d\u53c2\u6570\u3002 model = models . vgg16 () # \u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u597d\u7684\u53c2\u6570 model . load_state_dict ( torch . load ( 'model_weights.pth' )) model . eval () # \u5c06\u6a21\u578b\u8bbe\u7f6e\u4e3a\u6d4b\u8bd5\u6a21\u5f0f\uff0c\u907f\u514ddropout\u548cbatch normalization\u5bf9\u9884\u6d4b\u7ed3\u679c\u9020\u6210\u7684\u5f71\u54cd","title":"\u4fdd\u5b58\u548c\u52a0\u8f7d\u6a21\u578b"},{"location":"model/#_14","text":"import os import matplotlib.pyplot as plt from torchvision.transforms import ToTensor import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets , transforms # \u8bad\u7ec3\u6570\u636e\u96c6 training_data = datasets . FashionMNIST ( root = \"data\" , train = True , download = True , transform = ToTensor () # \u5bf9\u6837\u672c\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u8f6c\u6362\u4e3a\u5f20\u91cf\u6570\u636e ) # \u6d4b\u8bd5\u6570\u636e\u96c6 test_data = datasets . FashionMNIST ( root = \"data\" , train = False , download = True , transform = ToTensor () # \u5bf9\u6837\u672c\u6570\u636e\u8fdb\u884c\u5904\u7406\uff0c\u8f6c\u6362\u4e3a\u5f20\u91cf\u6570\u636e ) # \u6807\u7b7e\u5b57\u5178\uff0c\u4e00\u4e2akey\u952e\u5bf9\u5e94\u4e00\u4e2alabel labels_map = { 0 : \"T-Shirt\" , 1 : \"Trouser\" , 2 : \"Pullover\" , 3 : \"Dress\" , 4 : \"Coat\" , 5 : \"Sandal\" , 6 : \"Shirt\" , 7 : \"Sneaker\" , 8 : \"Bag\" , 9 : \"Ankle Boot\" , } # \u8bbe\u7f6e\u753b\u5e03\u5927\u5c0f # figure = plt.figure(figsize=(8, 8)) # cols, rows = 3, 3 # for i in range(1, cols * rows + 1): # # \u968f\u673a\u751f\u6210\u4e00\u4e2a\u7d22\u5f15 # sample_idx = torch.randint(len(training_data), size=(1,)).item() # # \u83b7\u53d6\u6837\u672c\u53ca\u5176\u5bf9\u5e94\u7684\u6807\u7b7e # img, label = training_data[sample_idx] # figure.add_subplot(rows, cols, i) # # \u8bbe\u7f6e\u6807\u9898 # plt.title(labels_map[label]) # # \u4e0d\u663e\u793a\u5750\u6807\u8f74 # plt.axis(\"off\") # # \u663e\u793a\u7070\u5ea6\u56fe # plt.imshow(img.squeeze(), cmap=\"gray\") # plt.show() # \u8bad\u7ec3\u6570\u636e\u52a0\u8f7d\u5668 train_dataloader = DataLoader ( dataset = training_data , # \u8bbe\u7f6e\u6279\u91cf\u5927\u5c0f batch_size = 64 , # \u6253\u4e71\u6837\u672c\u7684\u987a\u5e8f shuffle = True ) # \u6d4b\u8bd5\u6570\u636e\u52a0\u8f7d\u5668 test_dataloader = DataLoader ( dataset = test_data , batch_size = 64 , shuffle = True ) # \u5c55\u793a\u56fe\u7247\u548c\u6807\u7b7e # train_features, train_labels = next(iter(train_dataloader)) # print(f\"Feature batch shape: {train_features.size()}\") # print(f\"Labels batch shape: {train_labels.size()}\") # img = train_features[0].squeeze() # label = train_labels[0] # plt.imshow(img, cmap=\"gray\") # plt.show() # print(f\"Label: {label}\") # \u6a21\u578b\u5b9a\u4e49 class NeuralNetwork ( nn . Module ): def __init__ ( self ): super ( NeuralNetwork , self ) . __init__ () self . flatten = nn . Flatten () self . linear_relu_stack = nn . Sequential ( nn . Linear ( in_features = 28 * 28 , out_features = 512 ), nn . ReLU (), nn . Linear ( in_features = 512 , out_features = 512 ), nn . ReLU (), nn . Linear ( in_features = 512 , out_features = 10 ), ) def forward ( self , x ): x = self . flatten ( x ) logits = self . linear_relu_stack ( x ) return logits # \u4f18\u5316\u6a21\u578b\u53c2\u6570 def train_loop ( dataloader , model , loss_fn , optimizer , device ): size = len ( dataloader . dataset ) for batch , ( X , y ) in enumerate ( dataloader ): X = X . to ( device ) y = y . to ( device ) # \u524d\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u9884\u6d4b\u503c pred = model ( X ) # \u8ba1\u7b97\u635f\u5931 loss = loss_fn ( pred , y ) # \u53cd\u5411\u4f20\u64ad\uff0c\u4f18\u5316\u53c2\u6570 optimizer . zero_grad () loss . backward () optimizer . step () if batch % 100 == 0 : loss , current = loss . item (), batch * len ( X ) print ( f \"loss: { loss : >7f } [ { current : >5d } / { size : >5d } ]\" ) # \u6d4b\u8bd5\u6a21\u578b\u6027\u80fd def test_loop ( dataloader , model , loss_fn , device ): size = len ( dataloader . dataset ) num_batches = len ( dataloader ) test_loss , correct = 0 , 0 with torch . no_grad (): for X , y in dataloader : X = X . to ( device ) y = y . to ( device ) # \u524d\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u9884\u6d4b\u503c pred = model ( X ) # \u8ba1\u7b97\u635f\u5931 test_loss += loss_fn ( pred , y ) . item () # \u8ba1\u7b97\u51c6\u786e\u7387 correct += ( pred . argmax ( 1 ) == y ) . type ( torch . float ) . sum () . item () test_loss /= num_batches correct /= size print ( f \"Test Error: \\n Accuracy: { ( 100 * correct ) : >0.1f } %, Avg loss: { test_loss : >8f } \\n \" ) if __name__ == '__main__' : device = \"cuda\" if torch . cuda . is_available () else \"cpu\" print ( f \"Using { device } device\" ) # \u5b9a\u4e49\u6a21\u578b model = NeuralNetwork () . to ( device ) # \u8bbe\u7f6e\u8d85\u53c2\u6570 learning_rate = 1e-3 batch_size = 64 epochs = 5 # \u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668 loss_fn = nn . CrossEntropyLoss () optimizer = torch . optim . SGD ( params = model . parameters (), lr = learning_rate ) # \u8bad\u7ec3\u6a21\u578b for t in range ( epochs ): print ( f \"Epoch { t + 1 } \\n -------------------------------\" ) train_loop ( train_dataloader , model , loss_fn , optimizer , device ) test_loop ( test_dataloader , model , loss_fn , device ) print ( \"Done!\" ) # \u4fdd\u5b58\u6a21\u578b torch . save ( model . state_dict (), 'model_weights.pth' )","title":"\u9644\u5b8c\u6574\u4ee3\u7801"}]}